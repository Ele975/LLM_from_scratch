{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YHMwozlPU3qJ",
        "fRNvSiF4W7TY",
        "5blUPwwt5RVl",
        "4yha-9FShOL6"
      ],
      "authorship_tag": "ABX9TyOntZL9ctmhBKzAIcpn3PW3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ele975/LLM_from_scratch/blob/development/LLM_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import data"
      ],
      "metadata": {
        "id": "YHMwozlPU3qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for tokenizer\n",
        "!pip install tiktoken\n",
        "\n",
        "# for weights loading\n",
        "!pip install tensorflow>=2.15.0\n",
        "!pip install tqdm>=4.66"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhi2OPmvkSoN",
        "outputId": "f6eb0220-bc83-4041-c439-b0f87bcfedc5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib.request\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "l9NLRG6JUhg_"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download small dataset for training"
      ],
      "metadata": {
        "id": "5lOiAz9RUUS4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fMXIVq_UITh",
        "outputId": "e6d2534c-a821-48b4-cee4-5e3460a32a6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('the-verdict.txt', <http.client.HTTPMessage at 0x7e7eadd58550>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "        \"the-verdict.txt\")\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "print('Total number of chars:', len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RQB2xecVfUe",
        "outputId": "15ba2697-fcf4-44cb-d6aa-f7a19dc4480b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of chars: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "fRNvSiF4W7TY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text tokenization (manual implementation not necessary for the code, theoretical explanation)"
      ],
      "metadata": {
        "id": "LFy95txaZeIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "# remove white spaces\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AynKolSQXI50",
        "outputId": "c30ac18f-ecce-487d-ebbd-6e9b34856a5e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary creation (manual implementation not necessary for the code, theoretical explanation)"
      ],
      "metadata": {
        "id": "YmEHqnpGZfs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate ordered set of unique tokens\n",
        "all_words = sorted(set(preprocessed))\n",
        "# add token for unkown words (not in the vocab) and for termination of documents when concatenation (inputs are concatenated and model should distinguish them)\n",
        "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "print(len(all_words))\n",
        "print(all_words[:30])\n",
        "print(all_words[-5:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A_UmEleZp_0",
        "outputId": "327c24b1-3e3e-44e0-cdb7-3aa6b0932ce6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1132\n",
            "['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed']\n",
            "['younger', 'your', 'yourself', '<|endoftext|>', '<|unk|>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate vocabulary\n",
        "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  if i < 50:\n",
        "    print(item)\n",
        "  else:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC5YOxYcaCLP",
        "outputId": "43e91216-f06d-4560-c6f9-e98b91ab93fe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class to convert from token to ID and vice-versa"
      ],
      "metadata": {
        "id": "mXhPlXPYhv2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    # inverse vocabulary permitting to map from int to token\n",
        "    self.int_to_str = {integer:token for token, integer in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    # unkown tag if token not in vocab\n",
        "    preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    # remove unnecessary spaces\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "BBXtKea8bqW6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usage example"
      ],
      "metadata": {
        "id": "xj17qUX5l7Im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = SimpleTokenizerV1(vocab)\n",
        "# text = \"\"\"\"It's the last he painted, you know,\"\n",
        "#        Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "\n",
        "# ids = tokenizer.encode(text)\n",
        "# print(ids)\n",
        "\n",
        "# print(tokenizer.decode(ids))\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)\n",
        "\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(text))\n",
        "print(tokenizer.decode(tokenizer.encode(text)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOLlKWsRlKwq",
        "outputId": "5feb7c78-a932-40a4-f936-bd44e806e885"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
            "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
            "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset class and dataloader (manual implementation not necessary for the code, theoretical explanation)\n",
        "Through sliding window with parameters as context size (length) and stride"
      ],
      "metadata": {
        "id": "TG3q_Yuy16Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BPE tokenizer already implemented (instead of the basic ones implemented above)"
      ],
      "metadata": {
        "id": "GbRrfLyn2Lnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "g0_fhqGRkhKp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkPW6jkO2OJV",
        "outputId": "cd424262-70a2-438a-c9b9-8e74d7ceaf8e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[:50]"
      ],
      "metadata": {
        "id": "F_OIXoID2qhz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# elements in the input (window' size)\n",
        "context_size = 4"
      ],
      "metadata": {
        "id": "bN7gloUc26c7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset class -> each element in the dataset is a sequence of tokens, generating a dataset of successive strings. These sequences have a gap between each other of value 'stride'."
      ],
      "metadata": {
        "id": "FOQQ-VpcfS7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      input_chunk = token_ids[i:i+max_length]\n",
        "      target_chunk = token_ids[i+1:i+1+max_length]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n"
      ],
      "metadata": {
        "id": "AECAgAEBWxyU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader -> contains input batch and target batch because of the implementation of GPTDatasetV1 that generate the input ids and target ids"
      ],
      "metadata": {
        "id": "oAN3mEOCfVed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size = batch_size,\n",
        "      shuffle = shuffle,\n",
        "      drop_last = drop_last,\n",
        "      num_workers = num_workers\n",
        "  )\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "pCVKkmm-aPew"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print first batch example"
      ],
      "metadata": {
        "id": "H6U4yojleqkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NYDkC5aesRb",
        "outputId": "9de2edd6-4903-4259-d12f-6672dbed559c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token embedding (manual implementation not necessary for the code, theoretical explanation)\n",
        "Transform tokens IDs into embedding vectors (embedding can be optimized). Necessary since vectors are a continuous representation and neural networks use backward propagation for training. The embedding layer has dimension (vocab_size x embedding_dim), since for each word in the vocabulary we'll have an embedding vector of size embedding_dim. This embedding matrix is optimized during the training of the model as part of the training itself. Given then the token ID, it is possible to retrieve from the embedding matrix the embedding vector for that specific token."
      ],
      "metadata": {
        "id": "pSbOBfUigb_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# size vocabulary BPE tokenizer\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "31QocKVZtTLO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate data loader"
      ],
      "metadata": {
        "id": "gd8TTc7luC3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "   stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
        "\n",
        "# use embedding layer to embed the tokens inside the first batch\n",
        "# retrieve embedding vectors given IDs\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JYJJ_nWuE_Z",
        "outputId": "3e875dd9-4a8a-4f92-b5be-d1ab971277ca"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n",
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add absolute embedding approach"
      ],
      "metadata": {
        "id": "ZKeaixGIvgnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "# indices for positions 0,1,2 .. context_length - 1, i.e. 0, 1, 2 .. context_length - 1\n",
        "# retrieve embedding vectors given IDs\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "print(pos_embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcgPcd15vr_R",
        "outputId": "b4d9f9f0-59c6-46ba-df66-99b1572ee4f5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add positional embeddings to basic embeddings = input embedding"
      ],
      "metadata": {
        "id": "yJF1saHJ3k0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings"
      ],
      "metadata": {
        "id": "moEBdFpK3n2x"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention mechanism"
      ],
      "metadata": {
        "id": "5blUPwwt5RVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Simplified version (not necessary for the code, theoretical explanation)"
      ],
      "metadata": {
        "id": "L6m69mj7BAkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding vectors of 'Your journey starts with one step'"
      ],
      "metadata": {
        "id": "fk4nWd_CBErf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "[0.57, 0.85, 0.64], # starts\n",
        "[0.22, 0.58, 0.33], # with\n",
        "[0.77, 0.25, 0.10], # one\n",
        "[0.05, 0.80, 0.55]] # step\n",
        ")\n"
      ],
      "metadata": {
        "id": "UcCOJELc_RJ0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computation attention score for a specific embedding vector (token) with respect to all the others -> dot product between selected token (embedded query token) and all the other embedding vectors"
      ],
      "metadata": {
        "id": "h0ix8lI1BKXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computing all attention score for all inputs, i.e. 6 values for each input\n",
        "attn_scores = inputs @ inputs.T\n",
        "print(attn_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU_Tjq-uBbF_",
        "outputId": "b77224f8-af64-4831-dea2-2170ecabde2d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalized attention scores, i.e. AS[2]/tot(AS) or with Softmax (more used since better managing of extreme values)"
      ],
      "metadata": {
        "id": "7rsZg1VMCXNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "print(\"Attention weights:\", attn_weights)\n",
        "print('All row sums:', attn_weights.sum(dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9khQtDtyFUl1",
        "outputId": "81d83bab-ecc6-44d9-fa24-56b33117db2f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention weights: sum of all input embedding multiplied with their attention score"
      ],
      "metadata": {
        "id": "0cO8raJgGD4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2tylNhyGJTL",
        "outputId": "f457c6c4-09d8-428b-9530-1a02a11094a2"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Self-attention (with trainable parameters) (not necessary for the code, theoretical explanation)"
      ],
      "metadata": {
        "id": "B9EZ4bYsL0CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = inputs[1]\n",
        "d_in = inputs.shape[1]\n",
        "d_out = 2"
      ],
      "metadata": {
        "id": "ioxI750dL5YF"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrices (query, key, value)"
      ],
      "metadata": {
        "id": "9k6ycLVMMGca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "# requires_grad should be true if we really use them since they must be updated during training\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n"
      ],
      "metadata": {
        "id": "-rjEC6oqMIxK"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch translate automatically x_2 to adjust the dimensions\n",
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "print(query_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bnMYGBdMhKo",
        "outputId": "f96c87fe-74f6-4e56-c208-b16191d5a910"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get all keys and values for all input"
      ],
      "metadata": {
        "id": "18OnJpAVN9Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPLep4ScN8e5",
        "outputId": "950d3328-4e75-405c-a843-cf917e9a1f0b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention score: dot product between query vector and key vector, i.e. attention score 2_2: query2.key2"
      ],
      "metadata": {
        "id": "FpOx0-HQOnZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPbuB-BvOvD8",
        "outputId": "c7fbf979-e485-4014-8da0-5e69829a589c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention score of all inputs with respect to input 2 (i.e. using query 2)"
      ],
      "metadata": {
        "id": "kGW-UJ2vPQ0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores_2 = query_2 @ keys.T\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txOA9IG_PUIX",
        "outputId": "b1c0c5ab-8acc-4ba4-fa25-90a35c30694c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention weights -> normalization and softmax -> divide attention scores by dividing them by the square root of the embedding dimension of the keys"
      ],
      "metadata": {
        "id": "dCe5KsvAQcAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2/d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDE1Rq_MQnBz",
        "outputId": "34fb71ae-b9ac-4338-d467-d8fb262b017f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context vector for input 2 -> weighted sum over the value vectors"
      ],
      "metadata": {
        "id": "rsKQZ1x-Q7gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtbwYRW2Q808",
        "outputId": "b331d9fa-0490-47a1-e15d-a80f2bd62760"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compact implementation using Python class for each input"
      ],
      "metadata": {
        "id": "TZWOMCUWSlX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_V1(nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "    # call superclass init since internal initialization logic should be executed for nn.Module\n",
        "    super().__init__()\n",
        "    # nn.Linear has optimized weight initialization scheme\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "  # x = all inputs\n",
        "  def forward(self, x):\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "    queries = self.W_query(x)\n",
        "\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "SpdWqrXHSomm"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to use it (example)"
      ],
      "metadata": {
        "id": "hQIr0v6-XHEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v1 = SelfAttention_V1(d_in, d_out)\n",
        "# returns the context vectors\n",
        "print(sa_v1(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXz-4QA9WvrJ",
        "outputId": "44f82645-0246-4239-dbd1-ff6eef94a15a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Causal attention (not necessary for the code, theoretical explanation)\n",
        "Set to 0 all attention weights associated with the tokens after the selected one (lower triangular matrix)"
      ],
      "metadata": {
        "id": "p5HozL7wZ4lW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = sa_v1.W_query(inputs)\n",
        "keys = sa_v1.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z-3OPzr51th",
        "outputId": "01a1b50f-6f95-4aea-ff03-235d271680b8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create lower triangular matrix"
      ],
      "metadata": {
        "id": "PyTiFSkj67ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = attn_weights.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "print(mask_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xqZQFA96xMa",
        "outputId": "3801937f-0677-4f7e-dad2-62162f2071e8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert attention weights values"
      ],
      "metadata": {
        "id": "QIKgkmEF6-uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masked_simple = attn_weights * mask_simple\n",
        "print(masked_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVB8GENk662q",
        "outputId": "74a40e69-b503-4a46-f25c-0329a4f4aa9d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Renormalize values to have sum = 1 for each row (divide each row values for the sum in each row)"
      ],
      "metadata": {
        "id": "QfcqMRxZ7LvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "\n",
        "print(masked_simple_norm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1QCwDN37OFt",
        "outputId": "ba37593c-8457-4f4e-b38d-f2ce3f0627a2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set 0s values to -inf -> softmax treat them as 0 probability -> more efficient masking trick"
      ],
      "metadata": {
        "id": "lZj5zxmD-CKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# diagonal = 1 -> start from the diagonal above the middle one\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "# put -inf to all values above lower triangular\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqmHK-lr9tLa",
        "outputId": "411c2e73-01a0-43d6-e355-dd550122cb56"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax for normalization"
      ],
      "metadata": {
        "id": "8R6jKNwKN4Jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16gKjMAaN7JQ",
        "outputId": "f11c0013-8652-47b2-8e62-22f5526b9151"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement dropout layer to avoid overfitting"
      ],
      "metadata": {
        "id": "MZu5W11gUiOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "# in general dropout used: 0.2/0.3\n",
        "dropout = torch.nn.Dropout(0.5)\n",
        "print(dropout(attn_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXg03PhlT5in",
        "outputId": "f51763f0-14a4-4e1a-ed9b-53e616db1469"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add an input of 6 tokens, for a total of 2 inputs"
      ],
      "metadata": {
        "id": "HBN5Aj5S-H_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtDdnuSA-DZN",
        "outputId": "713711f0-5a08-47a2-fb23-3529140f0cc6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n",
            "tensor([[[0.4300, 0.1500, 0.8900],\n",
            "         [0.5500, 0.8700, 0.6600],\n",
            "         [0.5700, 0.8500, 0.6400],\n",
            "         [0.2200, 0.5800, 0.3300],\n",
            "         [0.7700, 0.2500, 0.1000],\n",
            "         [0.0500, 0.8000, 0.5500]],\n",
            "\n",
            "        [[0.4300, 0.1500, 0.8900],\n",
            "         [0.5500, 0.8700, 0.6600],\n",
            "         [0.5700, 0.8500, 0.6400],\n",
            "         [0.2200, 0.5800, 0.3300],\n",
            "         [0.7700, 0.2500, 0.1000],\n",
            "         [0.0500, 0.8000, 0.5500]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self attention class with causal mask component and dropout"
      ],
      "metadata": {
        "id": "7Ko_iWuC-Lxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # mask has size context_length x context_length since we need to store the attention scores and attention weights for each token before the current one (lower triangular matrix)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length, diagonal=1)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "    queries = self.W_query(x)\n",
        "\n",
        "    # transpose last two dimensions of keys to enable matrix multiplication -> from (batch_size, tokens_nr, embedding_dim) to (batch_size, embedding_dim, )\n",
        "    attn_scores = queries @ keys.transpose(1,2)\n",
        "    # access the mask above saved as buffer -> not optimized during backpropagation but available during the forward pass (often with masks)\n",
        "    attn_scores.masked_fill(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SnlRQb5u-T9q"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Multi-head attention\n",
        "Multiple queries, keys and values in parallel permits to compute different attention weights and attention scores. The resulting context vectors are then concatenated."
      ],
      "metadata": {
        "id": "StFeRYjFc2VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    # create the desired number of heads using the class above that generates a single causal attention head\n",
        "    self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    # head(x) call the forward method of the class CausalAttention and returns the context vectors for a single head\n",
        "    return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "xMnfpcn2c8Pb"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    # the inputs are multiplied by the matrices Q,K,V generating the reduced q,v,k which have the same dimension of the output context vector.\n",
        "    # For the parallel computation, q,v,k are split across the multiple heads, thus the dimension of q,v,k should be at least # heads\n",
        "    assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    # view = reshape from [b, num_tokens, d_in] to [b, num_tokens, self.num_heads, self.head_dim] -> dimensions of k,q,v for each head\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    # transpose in order to compute separately the results for each head, pass from [b, num_tokens, self.num_heads, self.head_dim] to [b, num_tokens, self.head_dim, self.num_heads]\n",
        "    keys = keys.transpose(1, 2)\n",
        "    queries = queries.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(2,3)\n",
        "    # not always context_sizes correspond to num_tokens (last batch, last input can have less tokens), thus cut\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    # transpose again to pass from [b, num_tokens, self.head_dim, self.num_heads] to [b, self.head_dim, num_tokens, self.num_heads]\n",
        "    # permit to concanenate easier the results of the different heads\n",
        "    context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "    # combine head results\n",
        "    context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "\n",
        "    # raw concatenation is not enough for the models, thus this is used to refine the concatenation. Not mandatory but commonly used\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "IPr69yP4cNYm"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU-ipFmYfPr5",
        "outputId": "e58562c9-c92a-4c60-a2c1-a66d77a847a7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]],\n",
            "\n",
            "        [[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM model (GPT-like model)"
      ],
      "metadata": {
        "id": "4yha-9FShOL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"emb_dim\": 768,   # Embedding dimension\n",
        "    \"n_heads\": 12,  # Number of attention heads\n",
        "    \"n_layers\": 12,  # Number of layers\n",
        "    \"drop_rate\": 0.1,  # Dropout rate\n",
        "    \"qkv_bias\": False # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "-pz1qazmhUpk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT dummy model"
      ],
      "metadata": {
        "id": "RdgjM7iHVyzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyGPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "    # For now use DummyTransformerBlockas placeholder for actual transformer layers\n",
        "    self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg)for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "    # for now placeholder as normal layer\n",
        "    self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "    # map embedding layer vocab_size x emb_dim into output layer emb_dim x vocab_size, such emb_dim is projected to vocab dimension, and applying a sofmax we can get the\n",
        "    # next work that is the one with the highest probability\n",
        "    self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    # lookup in the tok_emb for the indices given in in_idx\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    # seq_len = context_len\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    # final results\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "KkhFD7pDCfQF"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer blocks not implemented yet"
      ],
      "metadata": {
        "id": "IIUNbunGV1Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyTransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "pYgk5xeECiGd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization not implemented yet"
      ],
      "metadata": {
        "id": "tXBToCvIV3p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyLayerNorm(nn.Module):\n",
        "  def __init__(self, normalized_shape, eps=1e-5):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "a45rAV5NUkOM"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "print(batch)\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gImNLRKwVxR8",
        "outputId": "82ee1427-3e4d-403a-bbb1-77c3c5e80e46"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([6109, 3626, 6100,  345]), tensor([6109, 1110, 6622,  257])]\n",
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits = model(batch)\n",
        "# each token has embedding size of 50'257 because is the number of tokens in the vocab, then we'll use a softmax to know which token with highest probability will be the next\n",
        "print(\"Output shape:\", logits.shape)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXn9QxdqYYzK",
        "outputId": "25f21e8b-0a4a-4254-8b6e-97a7861779d2"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
            "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
            "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
            "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
            "\n",
            "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
            "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
            "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
            "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Normalization layer\n",
        "Used to avoid explosion or vanishing of the gradient through standard deviation (subtract the mean and divide by square root of variance)"
      ],
      "metadata": {
        "id": "cmYzmFWlonDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "      mean = x.mean(dim=-1, keepdim=True)\n",
        "      var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "      norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "      return self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "id": "7yWlU2CYpDSm"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. FNN with GELU\n",
        "GELU with respect to ReLu is better for optimization.\n",
        "The FNN layer permits the model to generalize and learn better the data: despite the input and output dimension is the same, internally this dimension is expanded, which permit the exporation of a richer representation space."
      ],
      "metadata": {
        "id": "48SC_BfUt4Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "LGHZeFWKuA3s"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "                                GELU(),\n",
        "                                nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "wDzEEGZ9ubIR"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Shortcut connections\n",
        "Added between different layers to improve training performance, since they avoid gradient vanishing by skipping some layers. How? Adding inpute values to the output of certain layers"
      ],
      "metadata": {
        "id": "TNs2Zn_NIR-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = torch.tensor([[0.]])\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0_FfwguPanZ",
        "outputId": "aa8240b0-025b-4e07-eb50-fc5de7898a9e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "  def __init__(self, layer_sizes, use_shortcut):\n",
        "    super().__init__()\n",
        "    self.use_shortcut = use_shortcut\n",
        "    self.layers = nn.ModuleList([\n",
        "      nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
        "    ])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      layer_output = layer(x)\n",
        "      if self.use_shortcut and x.shape == layer_output.shape:\n",
        "        x = x + layer_output\n",
        "      else:\n",
        "        x = layer_output\n",
        "    return x"
      ],
      "metadata": {
        "id": "uwm-UGzpSSCA"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
        "sample_input = torch.tensor([[1., 0., -1.]])\n",
        "torch.manual_seed(123)\n",
        "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
        "    layer_sizes, use_shortcut=True\n",
        ")"
      ],
      "metadata": {
        "id": "4YL3H-wYS1xJ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_gradients(model, x):\n",
        "  output = model(x)\n",
        "  target = torch.tensor([[0.]])\n",
        "  loss = nn.MSELoss()\n",
        "  loss = loss(output, target)\n",
        "  loss.backward()\n",
        "  for name, param in model.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
      ],
      "metadata": {
        "id": "afucjrTVS4dH"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_gradients(model_without_shortcut, sample_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2ox9FM9TEjd",
        "outputId": "d27a3d6d-af66-4d89-c7a0-c7767e63c810"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
            "layers.1.0.weight has gradient mean of 0.20694105327129364\n",
            "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
            "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
            "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Transformer block\n",
        "Composed by multi-head attention, layer normalization, dropout, feed forward layers, GELU activation function"
      ],
      "metadata": {
        "id": "kCSQjh15mgt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(\n",
        "        d_in=cfg[\"emb_dim\"],\n",
        "        d_out=cfg[\"emb_dim\"],\n",
        "        context_length=cfg[\"context_length\"],\n",
        "        num_heads=cfg[\"n_heads\"],\n",
        "        dropout=cfg[\"drop_rate\"],\n",
        "        qkv_bias=cfg[\"qkv_bias\"]\n",
        "    )\n",
        "    self.ff = FeedForward(cfg)\n",
        "    # normalize the embedding dimension\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    # save input for attention shortcut\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    # shortcut for FNN\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "uUMtqgofmr30"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT architecture class"
      ],
      "metadata": {
        "id": "LFoBv4N2vsI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "    *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(\n",
        "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "    )\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "6oTfYoC1vuWR"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run model"
      ],
      "metadata": {
        "id": "dWnhVdBGyfPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-aKqfZnygSa",
        "outputId": "bbf1f852-f38c-4bfd-8aac-e95fb68d998d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
            "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
            "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
            "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
            "\n",
            "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
            "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
            "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
            "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute total number of trainable/trained parameters"
      ],
      "metadata": {
        "id": "LqarEVGSX7aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IoO4FG9X1ew",
        "outputId": "467a4a4c-8b42-4c2b-c6b6-efe581d4efe0"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 163,009,536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generation of possibly multiple tokens, whose length depends on the user.Multiple iteration of the generation of a next token permit the model to generate meaninful complete sentences.\n",
        "\n",
        "Approaches:\n",
        "1. **softmax** -> the model returns everytime the token with the highest probability. The result is always the same every time the model is used.\n",
        "\n",
        "2. **multinomial** -> the model returns most of the time the token with the highest probability, but not always. In this way, the originality of the output text is improved.\n",
        "\n",
        "3. **temperature scaling** -> divide logitcs by a value greater than 0. Higher is the value of teh temperature, higher will be the originality. Too high values are not recommended (> 2/3), since we can have nonsensical output test\n",
        "\n",
        "**top-k approach**: possibly used before temperature scaling or multinomial to avoid nonsensical text -> select top k tokens with highest probabilities after softmax, and set to -inf all other logits s.t. the resulting probability with the softmax is 0 for all the other tokens, then normalize to have all remaining probabilities summed up to 1 (automatically done using the softmax).\n"
      ],
      "metadata": {
        "id": "BmgAOTOMi1ws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple version without text originality (used in the model traning)"
      ],
      "metadata": {
        "id": "IgGgjx98TDb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# idx -> input with size (batch, token ids)\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "  for _ in range(max_new_tokens):\n",
        "    # crops current context size to supported context size by the model -> if model supports size 5 and context size is 10, maintain last 5 tokens as context\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "\n",
        "    # output of model is (batch, token, vocab), thus take last token\n",
        "    logits = logits[:, -1, :]\n",
        "\n",
        "    # temperature scaling\n",
        "\n",
        "    # temperature = 0.5\n",
        "    # logits = logits/temperature\n",
        "\n",
        "    probas = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # get idx of the next token that will be part of the next input given to the model\n",
        "    idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "    # multinomial option -> take one sample and convert to tensor\n",
        "    # idx_text = torch.multinomial(probas, num_sample=1).item()\n",
        "\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "  return idx\n"
      ],
      "metadata": {
        "id": "hWotAydHf9bT"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete version with text originality (not used in the model training)"
      ],
      "metadata": {
        "id": "0qi3wzoaTEtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "    logits = logits[:, -1, :]\n",
        "    # choose if possible top k tokens\n",
        "    if top_k is not None:\n",
        "      top_logits, _ = torch.topk(logits, top_k)\n",
        "      min_val = top_logits[:, -1]\n",
        "      # set all other logits to -inf\n",
        "      logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device),logits)\n",
        "\n",
        "    if temperature > 0.0:\n",
        "      logits = logits / temperature\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "    else:\n",
        "      idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "    if idx_next == eos_id:\n",
        "      break\n",
        "\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "  return idx\n"
      ],
      "metadata": {
        "id": "E1LLvag4TKWt"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = \"Hello, I am\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "# add batch dimension (instead of (4), we get (1,4))\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "# put model in eval mode to remove layers used only for training (e.g. dropout layers)\n",
        "model.eval()\n",
        "\n",
        "out = generate_text_simple(\n",
        "  model=model,\n",
        "  idx=encoded_tensor,\n",
        "  max_new_tokens=6,\n",
        "  context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output:\", out)\n",
        "print(\"Output length:\", len(out[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N0Z3QI3kWlN",
        "outputId": "8fe164c4-cf56-4698-a46e-9c23f049e39c"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
            "Output length: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversion from GPT output to token\n",
        "The result of GPT model is a matrix. To get the predicted token, it is required to extract the last vector, apply the softmax to get probabilities, identify the index associated with the highest probability which is also the token ID, and get the token given its specific ID."
      ],
      "metadata": {
        "id": "nAEsA6tClLCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSDQ5g1vlC1R",
        "outputId": "173fd734-45f9-4b43-fa43-236403f0c8a4"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am Featureiman Byeswickattribute argue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretraining"
      ],
      "metadata": {
        "id": "vDlw791vbaj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shorter context_length\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwNCnu99ea2g",
        "outputId": "d941f8e7-1a7d-4ba8-c8b9-a60c7ce75cfb"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions to convert from tokens to tokens IDs and opposite"
      ],
      "metadata": {
        "id": "P1hMfooScnlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "  # insert batch dimension\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "  # remove batch dimension\n",
        "  flat = token_ids.squeeze(0)\n",
        "  return tokenizer.decode(flat.tolist())"
      ],
      "metadata": {
        "id": "qn5Gi8IIct6a"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try functions"
      ],
      "metadata": {
        "id": "ayYPSjrQepXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model = model,\n",
        "    idx = text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens = 10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amptzUZyeqWd",
        "outputId": "8b490c90-1be7-4953-eb6b-a09a620d4368"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you rentingetic wasnم refres RexMeCHicular stren\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss computation without training\n",
        "Load small dataset 'The verdict' to save time"
      ],
      "metadata": {
        "id": "nHl2tYYblXgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"the-verdict.txt\"\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "  text_data = f.read()"
      ],
      "metadata": {
        "id": "ceYmvl3ZlciV"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get text length"
      ],
      "metadata": {
        "id": "zRBMCBPBlobo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_chars = len(text_data)\n",
        "total_tok = tokenizer.encode(text_data)\n",
        "print(\"Total characters:\", total_chars)\n",
        "print(\"Total tokens:\", len(total_tok))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9k9Oisqlpvs",
        "outputId": "2863e365-63ea-4d31-fd52-e30998ca33fd"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 20479\n",
            "Total tokens: 5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split dataset into training set and validation set with ratio of 90% and 10%"
      ],
      "metadata": {
        "id": "fbzK51VpmrN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]"
      ],
      "metadata": {
        "id": "6Dkzv9UBmwQS"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loader -> contains for each element input batch and target batch"
      ],
      "metadata": {
        "id": "Hx4OtxfcnjF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size = batch_size,\n",
        "      shuffle = shuffle,\n",
        "      drop_last = drop_last,\n",
        "      num_workers = num_workers\n",
        "  )\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "e7zNWXYLnkiD"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "Sjp8ShVhnmDc"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "dBJGKcRDntdw"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute loss function for a single batch"
      ],
      "metadata": {
        "id": "Njuc6quMo5MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "  # transferring data to a device permit to transfer data to GPU\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "  logits = model(input_batch)\n",
        "  # logits.flatten(0,1) -> merge dimensions 0 and 1 into a single one, from (batch_dim, nr_tokens, vocab_dim) to (batch_dim*nr_tokens, vocab_dim)\n",
        "  loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "KequA5g3o4Xs"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute loss for all batches of traning and validation set"
      ],
      "metadata": {
        "id": "NgLfo8DZwNAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches = None):\n",
        "  total_loss = 0.\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "  # if num_batches not specified\n",
        "  elif num_batches is None:\n",
        "    # dataloader returns data grouped by batches\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i < num_batches:\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      # sum loss over batches\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  # average loss over all batches\n",
        "  return total_loss/num_batches\n"
      ],
      "metadata": {
        "id": "0JhKxsBtwO5K"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute loss (high since no training performed yet)"
      ],
      "metadata": {
        "id": "OYf-UF8_9p0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device)\n",
        "  val_loss = calc_loss_loader(val_loader, model, device)\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ViLgvTO8lJy",
        "outputId": "f5daea10-df70-415e-b248-a286bab22b1a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.987583584255642\n",
            "Validation loss: 10.98110580444336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Pretraining\n",
        "traning:\n",
        "1. iterate over each epoch (complete iteration over all input data)\n",
        "2. iterate over each batch\n",
        "3. reset loss gradients for each batch (batches are processed independently and their loss shouldn't be summed up).\n",
        "4. compute loss on current batch (forward pass)\n",
        "5. compute backpropagation (backward pass) to compute gradient of the loss with respect to the parameters (weights, biases)\n",
        "6. update parameters based on the gradients get in the previous step\n",
        "7. (print training and validation losses for tracking progress)\n",
        "\n",
        "Evaluating the validation set within the training process permit to understand how the model perform on unseen data in order to use techniques to improve precision if needed (modify hyperparameter such as batch size, learning rate, etc. ) and can implement strategies such as early stopping to avoid a biased model on training data (overfitting). In addition spikes in validation data indicates issues in the training process"
      ],
      "metadata": {
        "id": "yyKaxZo19T5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
        "  # track losses and tokens seen\n",
        "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "  # global_step initialized with -1 so that it can work as index\n",
        "  tokens_seen, global_step = 0, -1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      # compute loss gradients\n",
        "      loss.backward()\n",
        "      # update model parameters depending on loss gradients\n",
        "      optimizer.step()\n",
        "      tokens_seen += input_batch.numel()\n",
        "      global_step += 1\n",
        "\n",
        "      # eval_freq is the frequency on which we evaluate the model on a validation or test set (if 5, each 5 batches we evaluate the model), optional to see improvements\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(tokens_seen)\n",
        "\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "              f\"Train loss {train_loss:.3f}, \"\n",
        "              f\"Val loss {val_loss:.3f}\"\n",
        "              )\n",
        "\n",
        "    generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "  return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7fLmADJC9Tdn"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation: compute loss over training and evaluation sets ensuring model is in evaluation mode without dropout  "
      ],
      "metadata": {
        "id": "UcVW_1q1JGM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "  model.eval()\n",
        "  # not required in evaluation and save computational space\n",
        "  with torch.no_grad():\n",
        "    # compute loss for multiple batches to have more stable evaluation\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    # swap again to training mode for the training process in train_model_simple()\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "4MFjp0QJGeoc"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model by taking a text snippet and passing to the model"
      ],
      "metadata": {
        "id": "ePvsVqp0v0ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start_context = text snippet for evaluation purposes\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "  model.eval()\n",
        "  # in the positional embedding weights the first shape is the context size\n",
        "  context_size = model.pos_emb.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "    # use the model in order to get the predicted next tokens starting from start_context\n",
        "    token_ids = generate_text_simple(model=model, idx=encoded,max_new_tokens=50, context_size=context_size)\n",
        "  decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "  print(decoded_text.replace(\"\\n\", \" \"))\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "6yhf8ig7xvXE"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "FIaRTY0D0RHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(\n",
        "     model.parameters(),\n",
        "    lr=0.0004, weight_decay=0.1\n",
        ")\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer,\n",
        "    device,num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0agY0tuT0SZf",
        "outputId": "f9e0c181-902e-40e5-8365-e00ba31f15b3"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.783, Val loss 9.927\n",
            "Ep 1 (Step 000005): Train loss 7.985, Val loss 8.335\n",
            "Every effort moves you,,,,,,,,,,,,.                                     \n",
            "Ep 2 (Step 000010): Train loss 6.753, Val loss 7.048\n",
            "Ep 2 (Step 000015): Train loss 6.114, Val loss 6.573\n",
            "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n",
            "Ep 3 (Step 000020): Train loss 5.525, Val loss 6.490\n",
            "Ep 3 (Step 000025): Train loss 5.324, Val loss 6.387\n",
            "Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n",
            "Ep 4 (Step 000030): Train loss 4.761, Val loss 6.360\n",
            "Ep 4 (Step 000035): Train loss 4.461, Val loss 6.258\n",
            "Every effort moves you of the to the picture--as of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n",
            "Ep 5 (Step 000040): Train loss 3.833, Val loss 6.196\n",
            "Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n",
            "Ep 6 (Step 000045): Train loss 3.352, Val loss 6.139\n",
            "Ep 6 (Step 000050): Train loss 2.861, Val loss 6.112\n",
            "Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       \n",
            "Ep 7 (Step 000055): Train loss 2.347, Val loss 6.138\n",
            "Ep 7 (Step 000060): Train loss 2.084, Val loss 6.179\n",
            "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n",
            "Ep 8 (Step 000065): Train loss 1.521, Val loss 6.176\n",
            "Ep 8 (Step 000070): Train loss 1.272, Val loss 6.178\n",
            "Every effort moves you?\" \"I didn't bear the picture--I told me.  \"I looked up, and went on groping and Mrs. I was back the head to look up at the honour being _mine_--because he was when I\n",
            "Ep 9 (Step 000075): Train loss 1.000, Val loss 6.277\n",
            "Ep 9 (Step 000080): Train loss 0.718, Val loss 6.281\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 10 (Step 000085): Train loss 0.506, Val loss 6.325\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot training and validation loss -> the model overfit due to the small dataset and the absence of techniques to improve it, but this is a very basic training only for demonstration purposes"
      ],
      "metadata": {
        "id": "RRGUPINcJfn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "  ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "  ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "  ax1.set_xlabel(\"Epochs\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend(loc=\"upper right\")\n",
        "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "  ax2 = ax1.twiny()\n",
        "  ax2.plot(tokens_seen, train_losses, alpha=0)\n",
        "  ax2.set_xlabel(\"Tokens seen\")\n",
        "  fig.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "DRzAOPRMJca0"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "9P_9tlULJ39F",
        "outputId": "903ce8ae-9a85-4d03-80de-ddc8934b896d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model in order to not execute the training everytime (computational advantage) such that even if the session is closed the model will be still available. Two options:\n",
        "1. save the model only -> if we don't perform any additional training (only inference)\n",
        "2. save the model and the optimizer -> if additional training is performed, the optimizer contains important parameters that are needed in order to have a correct convergence"
      ],
      "metadata": {
        "id": "wqLmwvNhpVT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # save onyl the model\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# # save the model and the optimizer\n",
        "# torch.save({\n",
        "#     \"model_state_dict\": model.state_dict(),\n",
        "#     \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "#     },\n",
        "#     \"model_and_optimizer.pth\"\n",
        "# )"
      ],
      "metadata": {
        "id": "vAF0eVwrpifv"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model (in general this is done in another notebook in order to not run the training again)"
      ],
      "metadata": {
        "id": "OKnhK_LWrgFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"model.pth\", map_location=device)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
        "\n",
        "# checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
        "# model = GPTModel(GPT_CONFIG_124M)\n",
        "# model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "# optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "# # if we save also the optimizer, this means we want to train it again thus set train state\n",
        "# model.train();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4o_L2-IrpcQ",
        "outputId": "02eda0ea-737a-4b08-f07a-fae9e4c453e1"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-94-38373285fd4a>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\"model.pth\", map_location=device)\n",
            "<ipython-input-94-38373285fd4a>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pretrained weights of GPT2\n",
        "Since if the device is computationally limited the training is not effective, it is possible to load the weights of the GPT-2 models obtained during pretraining."
      ],
      "metadata": {
        "id": "4ACJJiensUu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get gpt_download.py\n",
        "\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split('/')[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNTOtHbGsUes",
        "outputId": "5c76e6de-a40e-4294-9404-e79c575a6cfc"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x7e7e83b70fa0>)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load GPT-2 architecture settings and parameters"
      ],
      "metadata": {
        "id": "VeEKbSxttyul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrFd1PJUzELf",
        "outputId": "f3e7b67d-0279-4e36-f474-cc99f98ca3c7"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 35.9kiB/s]\n",
            "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 2.48MiB/s]\n",
            "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 32.0kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:18<00:00, 27.2MiB/s]\n",
            "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 3.10MiB/s]\n",
            "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 1.71MiB/s]\n",
            "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 1.35MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter dictionary keys:\", params.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TU2iNEDxWrC",
        "outputId": "de5147c1-340b-41cc-c6f9-e8b004259feb"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All configurations for GPT-2 models (small, medium, large, xl)"
      ],
      "metadata": {
        "id": "RrBwdvQ1vD6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_configs = {\n",
        "  \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "  \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "  \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "  \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}"
      ],
      "metadata": {
        "id": "043ICH82vIzd"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update our previous config ('GPT_CONFIG_124M') with new parameters"
      ],
      "metadata": {
        "id": "m65inDalvP8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2-small (124M)\"\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "\n",
        "NEW_CONFIG.update({\"context_length\": 1024})\n",
        "# not used anymore but need to match the settings employed\n",
        "NEW_CONFIG.update({\"qkv_bias\": True})"
      ],
      "metadata": {
        "id": "F-6a68mNvVtG"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create new GPT instance with new parameters, however the initial weights are random"
      ],
      "metadata": {
        "id": "h3OIegbWwN5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWVUmYN9wQll",
        "outputId": "b00edd85-9868-463f-cba1-0620b2eb25dc"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to check that left and right tensors have same shape (check if assignment of data is correct)"
      ],
      "metadata": {
        "id": "5Bk4jGCjwuDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "  if left.shape != right.shape:\n",
        "    raise ValueError(f\"Shape mismatch. Left: {left.shape}, \" \"Right: {right.shape}\")\n",
        "  return torch.nn.Parameter(torch.tensor(right))"
      ],
      "metadata": {
        "id": "Sks2FRY6w7wt"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign loaded parameters to the new model with the new configuration. Complex function and based on some guessing since GPT used different names. It is possible to load the pretrained GPT model without manually assign all parameters, but this is for demonstration purpose (i.e. model = GPT2Model.from_pretrained(\"gpt2\"))"
      ],
      "metadata": {
        "id": "wIaw5i4pxAJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_weights_into_gpt(gpt, params):\n",
        "  # assign positional and token embeddings\n",
        "  gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "  gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "  # iteration over all transformer blocks\n",
        "  for b in range(len(params[\"blocks\"])):\n",
        "    # query, key and value components for self attention mechanism\n",
        "    # split concatenated weights for q,k,v into 3 components q_w, k_w, v_w, transpose and assign\n",
        "    q_w, k_w, v_w = np.split(\n",
        "        (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "    gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "        gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "    gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "        gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "    gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "        gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "    # same for biases\n",
        "    q_b, k_b, v_b = np.split(\n",
        "        (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "    gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "        gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "    gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "        gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "    gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "        gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "    # output projection\n",
        "    gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "        gpt.trf_blocks[b].att.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "    gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "        gpt.trf_blocks[b].att.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "    # feedforward nn\n",
        "    gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "    gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "    gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "    gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "    # layer normalization, where scale='g' for gain and shift='b' for bias\n",
        "    gpt.trf_blocks[b].norm1.scale = assign(\n",
        "        gpt.trf_blocks[b].norm1.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "    gpt.trf_blocks[b].norm1.shift = assign(\n",
        "        gpt.trf_blocks[b].norm1.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "    gpt.trf_blocks[b].norm2.scale = assign(\n",
        "        gpt.trf_blocks[b].norm2.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "    gpt.trf_blocks[b].norm2.shift = assign(\n",
        "        gpt.trf_blocks[b].norm2.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "  # final layer normalization and output head\n",
        "  gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "  gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "  gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "p-4eY21-xGqY"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl1nvY9eSsk0",
        "outputId": "8713ff0a-109c-477b-86ea-f5d7ea010348"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use model with loaded parameters to see token prediction and check that output text is coherent."
      ],
      "metadata": {
        "id": "BYuIlWe3S2JM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "token_ids = generate(\n",
        "model=gpt,\n",
        "idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device), max_new_tokens=25,\n",
        "context_size=NEW_CONFIG[\"context_length\"],\n",
        "top_k=50,\n",
        "temperature=1.5\n",
        ")\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GGj1TNCS0lx",
        "outputId": "43555094-d6e5-4559-bda3-bf927d14c29e"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you toward an equal share for each vote plus half. Inequality is often not an accurate representation of human worth; to know the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mpbHQPrnU0Lk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning (spam detection)\n",
        "After the pretraining where the model learns how to perdict next tokens in a coherent way, LLMs can be fine-tuned to perform required task such as classification, question answering, sentiment analysis, text generation, etc. The fine-tuning can be divided into:\n",
        "1. fine-tuning for classification: perform classification given an input such as spam detection, element recognition given images (tumors, object detection), text classification, etc. More specific than other fine-tuning since it is specialized in performing only the classification it saw only during the training phase\n",
        "\n",
        "2. fine-tuning to follow instruction: fine-tune model such that given some instructions (in form of prompts) in natural language it is able to understand and execute the required different tasks. E.g. \"**Translate in German** 'text to translate' \". Can perform different tasks"
      ],
      "metadata": {
        "id": "jFDTH4KYUMX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preprocessing"
      ],
      "metadata": {
        "id": "9vIrDo7oYAW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset for spam detection"
      ],
      "metadata": {
        "id": "cIgvJVqMYc_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "# name of downloaded file\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "# where the unzipped file is stored\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "# create path pointing to sms_spam_collection appending SMSSpamCollection to the path -> after the unzip the dataset will be called SMSSpamCollection.tsv\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "IOZ6BsM9YjUt"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "  if data_file_path.exists():\n",
        "    print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
        "    return\n",
        "  # download file (write in zip_path the file read from url contained in 'response')\n",
        "  with urllib.request.urlopen(url) as response:\n",
        "    with open(zip_path, \"wb\") as out_file:\n",
        "        out_file.write(response.read())\n",
        "  # unzip file\n",
        "  with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "  # add .tsv file extension\n",
        "  original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "  # add .tsv extension\n",
        "  os.rename(original_file_path, data_file_path)\n",
        "  print(f\"File downloaded and saved as {data_file_path}\")\n"
      ],
      "metadata": {
        "id": "2VcteykjY0s7"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRdoCLeSb8x0",
        "outputId": "f4533bf5-804c-4910-ea2f-3dbde6b7a750"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load .tsv file in a dataframe"
      ],
      "metadata": {
        "id": "LJ4BqAoVcsqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# /t since it it tab-separated (TSV) to know how to separate columns\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"] )\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "6C6Za6YWcyqC",
        "outputId": "b95d743d-329a-4d6f-b3ba-d1bd881f41b0"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Label                                               Text\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...    ...                                                ...\n",
              "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
              "5568   ham               Will ü b going to esplanade fr home?\n",
              "5569   ham  Pity, * was in mood for that. So...any other s...\n",
              "5570   ham  The guy did some bitching but I acted like i'd...\n",
              "5571   ham                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6857cc46-4b74-42cd-af4c-fbea3a073235\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will ü b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6857cc46-4b74-42cd-af4c-fbea3a073235')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6857cc46-4b74-42cd-af4c-fbea3a073235 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6857cc46-4b74-42cd-af4c-fbea3a073235');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ed6698fc-5660-47c4-bba3-4d6e8663b892\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ed6698fc-5660-47c4-bba3-4d6e8663b892')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ed6698fc-5660-47c4-bba3-4d6e8663b892 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_41f6ddc3-137b-4a32-9579-b1968dfcc41a\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_41f6ddc3-137b-4a32-9579-b1968dfcc41a button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"K, makes sense, btw carlos is being difficult so you guys are gonna smoke while I go pick up the second batch and get gas\",\n          \"URGENT! Your mobile No *********** WON a \\u00a32,000 Bonus Caller Prize on 02/06/03! This is the 2nd attempt to reach YOU! Call 09066362220 ASAP! BOX97N7QP, 150ppm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0et9--zdPp1",
        "outputId": "27c699ff-c6e0-46ea-956f-2bbae215db11"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple dataset balancing since the two classes 'spam' and 'ham' have very different sizes -> done to avoid class bias and improve generalization (not performant with the small classes)"
      ],
      "metadata": {
        "id": "4D3tfHpslvUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_balanced_dataset(df):\n",
        "  num_spam = df[df[\"Label\"] == 'spam'].shape[0]\n",
        "  # sample randomly num_spam number of ham data to balance the two classes. Ensure reproducibility with random_state\n",
        "  ham_subset = df[df[\"Label\"] == 'ham'].sample(num_spam, random_state=123)\n",
        "  # concatenate new column of the ham subset with all spam\n",
        "  balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
        "  return balanced_df"
      ],
      "metadata": {
        "id": "paj_M9cImLuu"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3pzR57roimi",
        "outputId": "e05b0d0d-b516-4322-f267-bd93b8653407"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     747\n",
            "spam    747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert labels into integers 0 and 1"
      ],
      "metadata": {
        "id": "imCPtGGMIj7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
      ],
      "metadata": {
        "id": "6wbcFAAlImme"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data in dataset randomly in training set (70%), validation set (10%), test set (20%)"
      ],
      "metadata": {
        "id": "wxu0wDbmJA2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_split(df, train_frac, validation_frac):\n",
        "  # sample randomly data, since we take frac=1, we simply shuffle them. Drop old indices and reset them\n",
        "  df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "  # get indices of split\n",
        "  train_end = int(len(df) * train_frac)\n",
        "  validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "  train_df = df[:train_end]\n",
        "  validation_df = df[train_end:validation_end]\n",
        "  test_df = df[validation_end:]\n",
        "\n",
        "  return train_df, validation_df, test_df\n"
      ],
      "metadata": {
        "id": "fQner_naJJRS"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)"
      ],
      "metadata": {
        "id": "fYIM2OE2V1aG"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save datasets as CSV"
      ],
      "metadata": {
        "id": "7XoMIrKaWLga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ],
      "metadata": {
        "id": "iTdrTPP6WNRk"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create data loaders -> for the pretraining we had a sliding window that permit to had sentences of the same length, but in emails the lengths are different, thus:\n",
        "1. truncate all messages to the length of the shortest one -> cheaper but there is the possibility of information loss if the shortest message is really short\n",
        "\n",
        "2. add padding to all messages reaching the length of the longest one -> expensive but preserve information -> in this case we use this option with \"<|endoftext|>\" as padding token\n"
      ],
      "metadata": {
        "id": "0ohvFfZXXw-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamDataset(Dataset):\n",
        "  def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "    self.data = pd.read_csv(csv_file)\n",
        "    self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]]\n",
        "    if max_length is None:\n",
        "      self.max_length = self._longest_encoded_length()\n",
        "    else:\n",
        "      self.max_length = max_length\n",
        "    self.encoded_texts = [encoded_text[:self.max_length]for encoded_text in self.encoded_texts]\n",
        "    self.encoded_texts = [encoded_text + [pad_token_id] *(self.max_length - len(encoded_text))for encoded_text in self.encoded_texts]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    encoded = self.encoded_texts[index]\n",
        "    label = self.data.iloc[index][\"Label\"]\n",
        "    return (torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def _longest_encoded_length(self):\n",
        "    max_length = 0\n",
        "    for encoded_text in self.encoded_texts:\n",
        "      encoded_length = len(encoded_text)\n",
        "      if encoded_length > max_length:\n",
        "        max_length = encoded_length\n",
        "    return max_length\n"
      ],
      "metadata": {
        "id": "gfmqR9TboOpt"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pad train, test and validation sets"
      ],
      "metadata": {
        "id": "KhCzmupO0mQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "edEhm1oGrrUV"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "GvQNoJut3NT8"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "93oL_Vqz3Os7"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev3AyWdE0xoU",
        "outputId": "cd222b53-1d0a-439f-ebcb-4d84ac5b659d"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate data loader -> each batch contains 8 input, where an input is an email text of 120 tokens and the label is 0 or 1 depending if spam or not"
      ],
      "metadata": {
        "id": "lyFMtRq_3giC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 0\n",
        "batch_size = 8\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0PHvz6l3hnF",
        "outputId": "8e9c5c1c-6072-48c1-a7a8-9837e6199093"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e7f717c79b0>"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")"
      ],
      "metadata": {
        "id": "xRjr85Ty3nRM"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ],
      "metadata": {
        "id": "PTh7YtOe3k2f"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ],
      "metadata": {
        "id": "G1baThSn3lx_"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check batch sizes if correct"
      ],
      "metadata": {
        "id": "blnUTZYQ4Gsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_batch, target_batch in train_loader:\n",
        "  pass\n",
        "print(\"Input batch dimensions:\", input_batch.shape)\n",
        "print(\"Label batch dimensions\", target_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8xHbSxc4I4K",
        "outputId": "b252ddd0-f8e8-456b-ede3-cb4b20ac91a9"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch dimensions: torch.Size([8, 120])\n",
            "Label batch dimensions torch.Size([8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print total batches in each set"
      ],
      "metadata": {
        "id": "Rx095dOMKuUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{len(train_loader)} training batches\")\n",
        "print(f\"{len(val_loader)} validation batches\")\n",
        "print(f\"{len(test_loader)} test batches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep7HHTwnKwLD",
        "outputId": "49c81fa7-f286-4597-d3c4-6eb51bd34090"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130 training batches\n",
            "19 validation batches\n",
            "38 test batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model setup"
      ],
      "metadata": {
        "id": "a_p6SeN8YJHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model initialization"
      ],
      "metadata": {
        "id": "YN0EN1moUwBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same configuration as for unlabeled data"
      ],
      "metadata": {
        "id": "sue2EVgeU2zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\""
      ],
      "metadata": {
        "id": "SOT3O2YcUydI"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"drop_rate\": 0.0,\n",
        "    \"qkv_bias\": True\n",
        "}"
      ],
      "metadata": {
        "id": "9Sem6hreU1Ov"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}"
      ],
      "metadata": {
        "id": "QqLL9VcaU6yt"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
      ],
      "metadata": {
        "id": "tiQ2NUdtU_vp"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download weights into gpt-2 model"
      ],
      "metadata": {
        "id": "GuuUBtCJVLWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5eC2I_8VMzz",
        "outputId": "26f44d77-c296-49a1-affd-0722208f158c"
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if model outputs coherent text, thus the parameters has been loaded correctly"
      ],
      "metadata": {
        "id": "9s4pbaxAV6gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = \"Every effort moves you\"\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_1, tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpLoS0YFV--Z",
        "outputId": "32dde915-c35c-4fa1-c245-7b74ef6832f2"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every effort moves you forward.\n",
            "\n",
            "The first step is to understand the importance of your work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification head\n",
        "Substitute the original output layer that maps to the vocabulary ('out_head') with a smaller output layer that maps to two classes only: 0 and 1 (not spam or spam). Without fine-tuning, with the new output layer the output of the model will be (batch_size, nr_tokens, vocab_dim), but the vocab_dim instead of being 50257 is now 2 (embedding dim) (for prediction of next token we take into consideration only the last token of the output).\n",
        "\n",
        "We freeze the model for the fine-tuning in order to not update the parameters -> keep the pre-trained weights intact and only train the new layers or the final layer to adapt the model to a new task or dataset"
      ],
      "metadata": {
        "id": "BHNA4TP-WETQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "rTh5uM_qWGZS"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change final layer. This one has requires_grad = True automatically, thus this will be the only layer updated during fine-tuning. However despite the fact this should be sufficient, by empirical experiments fine-tuning additional layers improve the performance of the model"
      ],
      "metadata": {
        "id": "JvjGqfKdYF9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "num_classes = 2\n",
        "\n",
        "model.out_head = torch.nn.Linear(\n",
        "    # = 768, output dimension of transformer blocks\n",
        "    in_features = BASE_CONFIG[\"emb_dim\"],\n",
        "    out_features = num_classes,\n",
        ")"
      ],
      "metadata": {
        "id": "ynOZjchuYHcR"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make trainable last transformer block and final LayerNorm"
      ],
      "metadata": {
        "id": "izxjQfBzZAOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "Jtz9MJ4PZfK4"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider only last token of the output for the classification. Why? Given the implementation of the attention mask that permit each token to have information about all preceeding tokens in the sequence, the last one is the one with most information. Using the softmax on these two values for each input text we get the probability to be spam or not spam, and we take the highest one as the correct class"
      ],
      "metadata": {
        "id": "ZFk6jfjhcIDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement evaluation utilities\n",
        "Compute classification loss and model accuracy (percentage of correct predictions)"
      ],
      "metadata": {
        "id": "gvrbmOmzr2IN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "  model.eval()\n",
        "  correct_predictions, num_examples = 0, 0\n",
        "  if num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "      num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i < num_batches:\n",
        "      input_batch = input_batch.to(device)\n",
        "      target_batch = target_batch.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        logits = model(input_batch)[:, -1, :]\n",
        "      predicted_labels = torch.argmax(logits, dim=-1)\n",
        "      num_examples += predicted_labels.shape[0]\n",
        "\n",
        "      correct_predictions += ((predicted_labels == target_batch).sum().item())\n",
        "    else:\n",
        "      break\n",
        "  return correct_predictions / num_examples"
      ],
      "metadata": {
        "id": "hDe0GO1Bsy-S"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function loss: cross-entropy for a single batch"
      ],
      "metadata": {
        "id": "v73MwIiQwpvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "  logits = model(input_batch)[:, -1, :]\n",
        "  loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "UT5wsEN2wrlw"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss for all batches"
      ],
      "metadata": {
        "id": "6kvhtEUtw3ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "  total_loss = 0.\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i < num_batches:\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss / num_batches"
      ],
      "metadata": {
        "id": "QFv8CLmLw5bF"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial loss"
      ],
      "metadata": {
        "id": "G9NvGuHuxWOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "\n",
        "val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
        "\n",
        "print(f\"Training loss: {train_loss:.3f}\")\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Test loss: {test_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZoOun9UxXKU",
        "outputId": "47a7eccb-9115-49c0-87e7-2b5fcc50ad16"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.784\n",
            "Validation loss: 0.795\n",
            "Test loss: 0.752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model fine-tuning and usage"
      ],
      "metadata": {
        "id": "LvlA9GHsYMZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier_simple(model, train_loader, val_loader, optimizer, device,num_epochs, eval_freq, eval_iter):\n",
        "  train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "  examples_seen, global_step = 0, -1\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      examples_seen += input_batch.shape[0]\n",
        "      global_step += 1\n",
        "\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "              f\"Train loss {train_loss:.3f}, \"\n",
        "              f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "    train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "    val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "\n",
        "    print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "    print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "    train_accs.append(train_accuracy)\n",
        "    val_accs.append(val_accuracy)\n",
        "  return train_losses, val_losses, train_accs, val_accs, examples_seen\n"
      ],
      "metadata": {
        "id": "BRZDmypE219M"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run fine-tuning computing the time needed"
      ],
      "metadata": {
        "id": "bI_Kw6udBrSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "num_epochs = 5"
      ],
      "metadata": {
        "id": "a7LV2lbFB9bh"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs=num_epochs, eval_freq=50,\n",
        "        eval_iter=5\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time)/60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E-Sgjn2A3WY",
        "outputId": "3f442dc1-dcca-4aea-952c-c7ee841bf9b4"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 0.589, Val loss 0.562\n",
            "Ep 1 (Step 000050): Train loss 0.389, Val loss 0.442\n",
            "Ep 1 (Step 000100): Train loss 0.288, Val loss 0.370\n",
            "Training accuracy: 92.50% | Validation accuracy: 92.50%\n",
            "Ep 2 (Step 000150): Train loss 0.286, Val loss 0.149\n",
            "Ep 2 (Step 000200): Train loss 0.063, Val loss 0.083\n",
            "Ep 2 (Step 000250): Train loss 0.061, Val loss 0.094\n",
            "Training accuracy: 95.00% | Validation accuracy: 97.50%\n",
            "Ep 3 (Step 000300): Train loss 0.149, Val loss 0.109\n",
            "Ep 3 (Step 000350): Train loss 0.093, Val loss 0.039\n",
            "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
            "Ep 4 (Step 000400): Train loss 0.021, Val loss 0.050\n",
            "Ep 4 (Step 000450): Train loss 0.082, Val loss 0.061\n",
            "Ep 4 (Step 000500): Train loss 0.139, Val loss 0.049\n",
            "Training accuracy: 97.50% | Validation accuracy: 97.50%\n",
            "Ep 5 (Step 000550): Train loss 0.134, Val loss 0.061\n",
            "Ep 5 (Step 000600): Train loss 0.057, Val loss 0.035\n",
            "Training accuracy: 100.00% | Validation accuracy: 100.00%\n",
            "Training completed in 60.60 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "  # plot training and validation values with respect to epochs\n",
        "  ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
        "  ax1.plot(epochs_seen, val_values, label=f\"Validation {label}\")\n",
        "  ax1.set_xlabel(\"Epoch\")\n",
        "  ax1.set_ylabel(label.capitalize())\n",
        "  ax1.legend()\n",
        "\n",
        "  # create new x-axis on same plot that overlay x1 but without interfere\n",
        "  ax2 = ax1.twiny()\n",
        "  ax2.plot(examples_seen, train_values, alpha=0)\n",
        "  ax2.set_xlabel(\"Examples seen\")\n",
        "\n",
        "  # adjust layout to avoid overlap between plot elements\n",
        "  fig.tight_layout()\n",
        "  # store plot\n",
        "  plt.savefig(f\"{label}-plot.pdf\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "dgUrbgGJEdsf"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot classification loss"
      ],
      "metadata": {
        "id": "dN5Uu_c2EaUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "Pr4D1ShzdRfj",
        "outputId": "4cfbadcb-7530-4d16-ee78-cfee5e39939f"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot classification accuracy"
      ],
      "metadata": {
        "id": "7gb8-rKnDQl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "o8HoG5W8DSMo",
        "outputId": "7e684f9c-d855-4b45-fbf2-e16516beb67d"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YHMwozlPU3qJ",
        "fRNvSiF4W7TY",
        "5blUPwwt5RVl",
        "4yha-9FShOL6",
        "vDlw791vbaj4",
        "jFDTH4KYUMX5",
        "9vIrDo7oYAW3"
      ],
      "authorship_tag": "ABX9TyPrYciSbOAbMEPkCM5BGewP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ele975/LLM_from_scratch/blob/main/LLM_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import data"
      ],
      "metadata": {
        "id": "YHMwozlPU3qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for tokenizer\n",
        "!pip install tiktoken\n",
        "\n",
        "# for weights loading\n",
        "!pip install tensorflow>=2.15.0\n",
        "!pip install tqdm>=4.66"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhi2OPmvkSoN",
        "outputId": "8acf49ba-c611-4761-f9d8-6da58f3cb641"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import urllib.request\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "import json\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from functools import partial\n",
        "from pathlib import Path\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "l9NLRG6JUhg_"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download small dataset for training"
      ],
      "metadata": {
        "id": "5lOiAz9RUUS4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fMXIVq_UITh",
        "outputId": "3a0822e4-905a-4767-af68-5e30409d7be5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('the-verdict.txt', <http.client.HTTPMessage at 0x7e0408e094e0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
        "        \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
        "        \"the-verdict.txt\")\n",
        "\n",
        "file_path = \"the-verdict.txt\"\n",
        "urllib.request.urlretrieve(url, file_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "print('Total number of chars:', len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RQB2xecVfUe",
        "outputId": "265c0da5-2d6a-420b-b8b8-bddd59ae2c6a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of chars: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "fRNvSiF4W7TY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text tokenization (manual implementation not necessary for the code, theoretical explanation)"
      ],
      "metadata": {
        "id": "LFy95txaZeIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "# remove white spaces\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AynKolSQXI50",
        "outputId": "63cf94db-7ccb-4a9f-caa9-922f1590584b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulary creation (manual implementation not necessary for the code, theoretical explanation)"
      ],
      "metadata": {
        "id": "YmEHqnpGZfs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate ordered set of unique tokens\n",
        "all_words = sorted(set(preprocessed))\n",
        "# add token for unkown words (not in the vocab) and for termination of documents when concatenation (inputs are concatenated and model should distinguish them)\n",
        "all_words.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "print(len(all_words))\n",
        "print(all_words[:30])\n",
        "print(all_words[-5:])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-A_UmEleZp_0",
        "outputId": "37de809d-d1e1-43fe-faa7-aed5ae071642"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1132\n",
            "['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed']\n",
            "['younger', 'your', 'yourself', '<|endoftext|>', '<|unk|>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate vocabulary\n",
        "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  if i < 50:\n",
        "    print(item)\n",
        "  else:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC5YOxYcaCLP",
        "outputId": "3fd68df6-9bc0-49b0-b450-12fc69a53c20"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Chicago', 25)\n",
            "('Claude', 26)\n",
            "('Come', 27)\n",
            "('Croft', 28)\n",
            "('Destroyed', 29)\n",
            "('Devonshire', 30)\n",
            "('Don', 31)\n",
            "('Dubarry', 32)\n",
            "('Emperors', 33)\n",
            "('Florence', 34)\n",
            "('For', 35)\n",
            "('Gallery', 36)\n",
            "('Gideon', 37)\n",
            "('Gisburn', 38)\n",
            "('Gisburns', 39)\n",
            "('Grafton', 40)\n",
            "('Greek', 41)\n",
            "('Grindle', 42)\n",
            "('Grindles', 43)\n",
            "('HAD', 44)\n",
            "('Had', 45)\n",
            "('Hang', 46)\n",
            "('Has', 47)\n",
            "('He', 48)\n",
            "('Her', 49)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Class to convert from token to ID and vice-versa"
      ],
      "metadata": {
        "id": "mXhPlXPYhv2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    # inverse vocabulary permitting to map from int to token\n",
        "    self.int_to_str = {integer:token for token, integer in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    # unkown tag if token not in vocab\n",
        "    preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in preprocessed]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    # remove unnecessary spaces\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "BBXtKea8bqW6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usage example"
      ],
      "metadata": {
        "id": "xj17qUX5l7Im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = SimpleTokenizerV1(vocab)\n",
        "# text = \"\"\"\"It's the last he painted, you know,\"\n",
        "#        Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "\n",
        "# ids = tokenizer.encode(text)\n",
        "# print(ids)\n",
        "\n",
        "# print(tokenizer.decode(ids))\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)\n",
        "\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(text))\n",
        "print(tokenizer.decode(tokenizer.encode(text)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOLlKWsRlKwq",
        "outputId": "1e36ea49-89af-4b48-df8a-a9c57ca389f5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n",
            "[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]\n",
            "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset class and dataloader (manual implementation not necessary for the code, theoretical explanation)\n",
        "Through sliding window with parameters as context size (length) and stride"
      ],
      "metadata": {
        "id": "TG3q_Yuy16Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BPE tokenizer already implemented (instead of the basic ones implemented above)"
      ],
      "metadata": {
        "id": "GbRrfLyn2Lnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "g0_fhqGRkhKp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "enc_text = tokenizer.encode(raw_text)\n",
        "print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkPW6jkO2OJV",
        "outputId": "97790e97-6ba6-4f75-b4ef-d28da6f7731e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_sample = enc_text[:50]"
      ],
      "metadata": {
        "id": "F_OIXoID2qhz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# elements in the input (window' size)\n",
        "context_size = 4"
      ],
      "metadata": {
        "id": "bN7gloUc26c7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset class -> each element in the dataset is a sequence of tokens, generating a dataset of successive strings. These sequences have a gap between each other of value 'stride'."
      ],
      "metadata": {
        "id": "FOQQ-VpcfS7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      input_chunk = token_ids[i:i+max_length]\n",
        "      target_chunk = token_ids[i+1:i+1+max_length]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]\n"
      ],
      "metadata": {
        "id": "AECAgAEBWxyU"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader -> contains input batch and target batch because of the implementation of GPTDatasetV1 that generate the input ids and target ids"
      ],
      "metadata": {
        "id": "oAN3mEOCfVed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size = batch_size,\n",
        "      shuffle = shuffle,\n",
        "      drop_last = drop_last,\n",
        "      num_workers = num_workers\n",
        "  )\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "pCVKkmm-aPew"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print first batch example"
      ],
      "metadata": {
        "id": "H6U4yojleqkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NYDkC5aesRb",
        "outputId": "fc4e9408-1488-4d6a-e6d1-b29e1366c8d2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token embedding (manual implementation not necessary for the code, theoretical explanation)\n",
        "Transform tokens IDs into embedding vectors (embedding can be optimized). Necessary since vectors are a continuous representation and neural networks use backward propagation for training. The embedding layer has dimension (vocab_size x embedding_dim), since for each word in the vocabulary we'll have an embedding vector of size embedding_dim. This embedding matrix is optimized during the training of the model as part of the training itself. Given then the token ID, it is possible to retrieve from the embedding matrix the embedding vector for that specific token."
      ],
      "metadata": {
        "id": "pSbOBfUigb_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# size vocabulary BPE tokenizer\n",
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "31QocKVZtTLO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate data loader"
      ],
      "metadata": {
        "id": "gd8TTc7luC3L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "   stride=max_length, shuffle=False\n",
        ")\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)\n",
        "\n",
        "# use embedding layer to embed the tokens inside the first batch\n",
        "# retrieve embedding vectors given IDs\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JYJJ_nWuE_Z",
        "outputId": "745a5296-4548-4554-ff27-041591860968"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n",
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add absolute embedding approach"
      ],
      "metadata": {
        "id": "ZKeaixGIvgnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "# indices for positions 0,1,2 .. context_length - 1, i.e. 0, 1, 2 .. context_length - 1\n",
        "# retrieve embedding vectors given IDs\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "print(pos_embeddings.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcgPcd15vr_R",
        "outputId": "28cb3504-a09d-4ede-d255-4f6815d17e3a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add positional embeddings to basic embeddings = input embedding"
      ],
      "metadata": {
        "id": "yJF1saHJ3k0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings"
      ],
      "metadata": {
        "id": "moEBdFpK3n2x"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention mechanism"
      ],
      "metadata": {
        "id": "5blUPwwt5RVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Simplified version (not necessary for the code, theoretical explanation)"
      ],
      "metadata": {
        "id": "L6m69mj7BAkQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding vectors of 'Your journey starts with one step'"
      ],
      "metadata": {
        "id": "fk4nWd_CBErf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "[0.57, 0.85, 0.64], # starts\n",
        "[0.22, 0.58, 0.33], # with\n",
        "[0.77, 0.25, 0.10], # one\n",
        "[0.05, 0.80, 0.55]] # step\n",
        ")\n"
      ],
      "metadata": {
        "id": "UcCOJELc_RJ0"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computation attention score for a specific embedding vector (token) with respect to all the others -> dot product between selected token (embedded query token) and all the other embedding vectors"
      ],
      "metadata": {
        "id": "h0ix8lI1BKXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computing all attention score for all inputs, i.e. 6 values for each input\n",
        "attn_scores = inputs @ inputs.T\n",
        "print(attn_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU_Tjq-uBbF_",
        "outputId": "2539ac27-f04b-4068-ec11-aae92ba0ff02"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "normalized attention scores, i.e. AS[2]/tot(AS) or with Softmax (more used since better managing of extreme values)"
      ],
      "metadata": {
        "id": "7rsZg1VMCXNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "print(\"Attention weights:\", attn_weights)\n",
        "print('All row sums:', attn_weights.sum(dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9khQtDtyFUl1",
        "outputId": "46d48a98-3609-44c4-e80c-c57be028cd2b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights: tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
            "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention weights: sum of all input embedding multiplied with their attention score"
      ],
      "metadata": {
        "id": "0cO8raJgGD4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vecs = attn_weights @ inputs\n",
        "print(all_context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2tylNhyGJTL",
        "outputId": "eac781f5-a463-4c37-ad92-7d927d1b7205"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Self-attention (with trainable parameters) (not necessary for the code, theoretical explanation)"
      ],
      "metadata": {
        "id": "B9EZ4bYsL0CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_2 = inputs[1]\n",
        "d_in = inputs.shape[1]\n",
        "d_out = 2"
      ],
      "metadata": {
        "id": "ioxI750dL5YF"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Matrices (query, key, value)"
      ],
      "metadata": {
        "id": "9k6ycLVMMGca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "# requires_grad should be true if we really use them since they must be updated during training\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n"
      ],
      "metadata": {
        "id": "-rjEC6oqMIxK"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch translate automatically x_2 to adjust the dimensions\n",
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "print(query_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bnMYGBdMhKo",
        "outputId": "ca398b57-37d5-486b-90d6-703d1d8b9ac4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get all keys and values for all input"
      ],
      "metadata": {
        "id": "18OnJpAVN9Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPLep4ScN8e5",
        "outputId": "964acbe9-92c0-4354-986a-59bd68ebf6ad"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention score: dot product between query vector and key vector, i.e. attention score 2_2: query2.key2"
      ],
      "metadata": {
        "id": "FpOx0-HQOnZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "print(attn_score_22)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPbuB-BvOvD8",
        "outputId": "9fb85f24-b5db-473d-c9a2-a8305d13e19e"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention score of all inputs with respect to input 2 (i.e. using query 2)"
      ],
      "metadata": {
        "id": "kGW-UJ2vPQ0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores_2 = query_2 @ keys.T\n",
        "print(attn_scores_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txOA9IG_PUIX",
        "outputId": "4d98bed4-e5be-4013-a64f-45af66a2628a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attention weights -> normalization and softmax -> divide attention scores by dividing them by the square root of the embedding dimension of the keys"
      ],
      "metadata": {
        "id": "dCe5KsvAQcAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_k = keys.shape[-1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2/d_k**0.5, dim=-1)\n",
        "print(attn_weights_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDE1Rq_MQnBz",
        "outputId": "09618556-2735-4b54-963f-309f151866dc"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context vector for input 2 -> weighted sum over the value vectors"
      ],
      "metadata": {
        "id": "rsKQZ1x-Q7gW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_vec_2 = attn_weights_2 @ values\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtbwYRW2Q808",
        "outputId": "2f6049d7-34dd-4128-9d53-4146e85ee589"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compact implementation using Python class for each input"
      ],
      "metadata": {
        "id": "TZWOMCUWSlX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention_V1(nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "    # call superclass init since internal initialization logic should be executed for nn.Module\n",
        "    super().__init__()\n",
        "    # nn.Linear has optimized weight initialization scheme\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "  # x = all inputs\n",
        "  def forward(self, x):\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "    queries = self.W_query(x)\n",
        "\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "SpdWqrXHSomm"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How to use it (example)"
      ],
      "metadata": {
        "id": "hQIr0v6-XHEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v1 = SelfAttention_V1(d_in, d_out)\n",
        "# returns the context vectors\n",
        "print(sa_v1(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXz-4QA9WvrJ",
        "outputId": "14fb0481-a7fc-4f7a-fa2a-413b20d15a5f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Causal attention (not necessary for the code, theoretical explanation)\n",
        "Set to 0 all attention weights associated with the tokens after the selected one (lower triangular matrix)"
      ],
      "metadata": {
        "id": "p5HozL7wZ4lW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = sa_v1.W_query(inputs)\n",
        "keys = sa_v1.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z-3OPzr51th",
        "outputId": "29f09118-0d4f-48c7-97ee-062ce9ca716b"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create lower triangular matrix"
      ],
      "metadata": {
        "id": "PyTiFSkj67ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = attn_weights.shape[0]\n",
        "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "print(mask_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xqZQFA96xMa",
        "outputId": "5634b718-6f3f-43f9-ea64-ced9be2cf3a5"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert attention weights values"
      ],
      "metadata": {
        "id": "QIKgkmEF6-uT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "masked_simple = attn_weights * mask_simple\n",
        "print(masked_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVB8GENk662q",
        "outputId": "8dd165f7-8e76-43df-802a-e13242606625"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Renormalize values to have sum = 1 for each row (divide each row values for the sum in each row)"
      ],
      "metadata": {
        "id": "QfcqMRxZ7LvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "\n",
        "print(masked_simple_norm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1QCwDN37OFt",
        "outputId": "6290c1bc-07ee-409c-c321-ae6d1210e7c3"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set 0s values to -inf -> softmax treat them as 0 probability -> more efficient masking trick"
      ],
      "metadata": {
        "id": "lZj5zxmD-CKg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# diagonal = 1 -> start from the diagonal above the middle one\n",
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "# put -inf to all values above lower triangular\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqmHK-lr9tLa",
        "outputId": "a2bd55ff-6726-4319-90a5-058840b09fc6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax for normalization"
      ],
      "metadata": {
        "id": "8R6jKNwKN4Jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16gKjMAaN7JQ",
        "outputId": "916fab90-c6f5-4e00-d1e2-9616aa0dd2ee"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement dropout layer to avoid overfitting"
      ],
      "metadata": {
        "id": "MZu5W11gUiOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "# in general dropout used: 0.2/0.3\n",
        "dropout = torch.nn.Dropout(0.5)\n",
        "print(dropout(attn_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXg03PhlT5in",
        "outputId": "140b2536-27fe-48a9-eef3-4b2972ca6da1"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add an input of 6 tokens, for a total of 2 inputs"
      ],
      "metadata": {
        "id": "HBN5Aj5S-H_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtDdnuSA-DZN",
        "outputId": "d8d823f1-fc62-40f0-f1e9-03a38661b532"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n",
            "tensor([[[0.4300, 0.1500, 0.8900],\n",
            "         [0.5500, 0.8700, 0.6600],\n",
            "         [0.5700, 0.8500, 0.6400],\n",
            "         [0.2200, 0.5800, 0.3300],\n",
            "         [0.7700, 0.2500, 0.1000],\n",
            "         [0.0500, 0.8000, 0.5500]],\n",
            "\n",
            "        [[0.4300, 0.1500, 0.8900],\n",
            "         [0.5500, 0.8700, 0.6600],\n",
            "         [0.5700, 0.8500, 0.6400],\n",
            "         [0.2200, 0.5800, 0.3300],\n",
            "         [0.7700, 0.2500, 0.1000],\n",
            "         [0.0500, 0.8000, 0.5500]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self attention class with causal mask component and dropout"
      ],
      "metadata": {
        "id": "7Ko_iWuC-Lxy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # mask has size context_length x context_length since we need to store the attention scores and attention weights for each token before the current one (lower triangular matrix)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length, diagonal=1)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    values = self.W_value(x)\n",
        "    queries = self.W_query(x)\n",
        "\n",
        "    # transpose last two dimensions of keys to enable matrix multiplication -> from (batch_size, tokens_nr, embedding_dim) to (batch_size, embedding_dim, )\n",
        "    attn_scores = queries @ keys.transpose(1,2)\n",
        "    # access the mask above saved as buffer -> not optimized during backpropagation but available during the forward pass (often with masks)\n",
        "    attn_scores.masked_fill(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SnlRQb5u-T9q"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Multi-head attention\n",
        "Multiple queries, keys and values in parallel permits to compute different attention weights and attention scores. The resulting context vectors are then concatenated."
      ],
      "metadata": {
        "id": "StFeRYjFc2VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    # create the desired number of heads using the class above that generates a single causal attention head\n",
        "    self.heads = nn.ModuleList([CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    # head(x) call the forward method of the class CausalAttention and returns the context vectors for a single head\n",
        "    return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "xMnfpcn2c8Pb"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    # the inputs are multiplied by the matrices Q,K,V generating the reduced q,v,k which have the same dimension of the output context vector.\n",
        "    # For the parallel computation, q,v,k are split across the multiple heads, thus the dimension of q,v,k should be at least # heads\n",
        "    assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    # view = reshape from [b, num_tokens, d_in] to [b, num_tokens, self.num_heads, self.head_dim] -> dimensions of k,q,v for each head\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    # transpose in order to compute separately the results for each head, pass from [b, num_tokens, self.num_heads, self.head_dim] to [b, num_tokens, self.head_dim, self.num_heads]\n",
        "    keys = keys.transpose(1, 2)\n",
        "    queries = queries.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(2,3)\n",
        "    # not always context_sizes correspond to num_tokens (last batch, last input can have less tokens), thus cut\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    # transpose again to pass from [b, num_tokens, self.head_dim, self.num_heads] to [b, self.head_dim, num_tokens, self.num_heads]\n",
        "    # permit to concanenate easier the results of the different heads\n",
        "    context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "    # combine head results\n",
        "    context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "\n",
        "    # raw concatenation is not enough for the models, thus this is used to refine the concatenation. Not mandatory but commonly used\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "IPr69yP4cNYm"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU-ipFmYfPr5",
        "outputId": "f016c47d-a531-4f1e-ee32-61e2a47976f7"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]],\n",
            "\n",
            "        [[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM model (GPT-like model)"
      ],
      "metadata": {
        "id": "4yha-9FShOL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"emb_dim\": 768,   # Embedding dimension\n",
        "    \"n_heads\": 12,  # Number of attention heads\n",
        "    \"n_layers\": 12,  # Number of layers\n",
        "    \"drop_rate\": 0.1,  # Dropout rate\n",
        "    \"qkv_bias\": False # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "-pz1qazmhUpk"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT dummy model"
      ],
      "metadata": {
        "id": "RdgjM7iHVyzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyGPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "    # For now use DummyTransformerBlockas placeholder for actual transformer layers\n",
        "    self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg)for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "    # for now placeholder as normal layer\n",
        "    self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "    # map embedding layer vocab_size x emb_dim into output layer emb_dim x vocab_size, such emb_dim is projected to vocab dimension, and applying a sofmax we can get the\n",
        "    # next work that is the one with the highest probability\n",
        "    self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    # lookup in the tok_emb for the indices given in in_idx\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    # seq_len = context_len\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    # final results\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "KkhFD7pDCfQF"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer blocks not implemented yet"
      ],
      "metadata": {
        "id": "IIUNbunGV1Nr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyTransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "pYgk5xeECiGd"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization not implemented yet"
      ],
      "metadata": {
        "id": "tXBToCvIV3p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyLayerNorm(nn.Module):\n",
        "  def __init__(self, normalized_shape, eps=1e-5):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "a45rAV5NUkOM"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "batch = []\n",
        "txt1 = \"Every effort moves you\"\n",
        "txt2 = \"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "print(batch)\n",
        "batch = torch.stack(batch, dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gImNLRKwVxR8",
        "outputId": "b0ebeba3-08aa-4799-b123-2c5652d18fb2"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([6109, 3626, 6100,  345]), tensor([6109, 1110, 6622,  257])]\n",
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits = model(batch)\n",
        "# each token has embedding size of 50'257 because is the number of tokens in the vocab, then we'll use a softmax to know which token with highest probability will be the next\n",
        "print(\"Output shape:\", logits.shape)\n",
        "print(logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXn9QxdqYYzK",
        "outputId": "de76b050-d47e-47f1-cb68-42f3e3b4cde2"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
            "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
            "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
            "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
            "\n",
            "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
            "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
            "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
            "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Normalization layer\n",
        "Used to avoid explosion or vanishing of the gradient through standard deviation (subtract the mean and divide by square root of variance)"
      ],
      "metadata": {
        "id": "cmYzmFWlonDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "      mean = x.mean(dim=-1, keepdim=True)\n",
        "      var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "      norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "      return self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "id": "7yWlU2CYpDSm"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. FNN with GELU\n",
        "GELU with respect to ReLu is better for optimization.\n",
        "The FNN layer permits the model to generalize and learn better the data: despite the input and output dimension is the same, internally this dimension is expanded, which permit the exporation of a richer representation space."
      ],
      "metadata": {
        "id": "48SC_BfUt4Wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))"
      ],
      "metadata": {
        "id": "LGHZeFWKuA3s"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "                                GELU(),\n",
        "                                nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "wDzEEGZ9ubIR"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Shortcut connections\n",
        "Added between different layers to improve training performance, since they avoid gradient vanishing by skipping some layers. How? Adding inpute values to the output of certain layers"
      ],
      "metadata": {
        "id": "TNs2Zn_NIR-Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = torch.tensor([[0.]])\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0_FfwguPanZ",
        "outputId": "6257c30e-dfb4-425e-8921-0473f6c33c45"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "  def __init__(self, layer_sizes, use_shortcut):\n",
        "    super().__init__()\n",
        "    self.use_shortcut = use_shortcut\n",
        "    self.layers = nn.ModuleList([\n",
        "      nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
        "      nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
        "    ])\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      layer_output = layer(x)\n",
        "      if self.use_shortcut and x.shape == layer_output.shape:\n",
        "        x = x + layer_output\n",
        "      else:\n",
        "        x = layer_output\n",
        "    return x"
      ],
      "metadata": {
        "id": "uwm-UGzpSSCA"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes = [3, 3, 3, 3, 3, 1]\n",
        "sample_input = torch.tensor([[1., 0., -1.]])\n",
        "torch.manual_seed(123)\n",
        "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
        "    layer_sizes, use_shortcut=True\n",
        ")"
      ],
      "metadata": {
        "id": "4YL3H-wYS1xJ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_gradients(model, x):\n",
        "  output = model(x)\n",
        "  target = torch.tensor([[0.]])\n",
        "  loss = nn.MSELoss()\n",
        "  loss = loss(output, target)\n",
        "  loss.backward()\n",
        "  for name, param in model.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
      ],
      "metadata": {
        "id": "afucjrTVS4dH"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_gradients(model_without_shortcut, sample_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2ox9FM9TEjd",
        "outputId": "815bc269-e192-4b0f-b01f-0b41a56d54d6"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.22169791162014008\n",
            "layers.1.0.weight has gradient mean of 0.20694105327129364\n",
            "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
            "layers.3.0.weight has gradient mean of 0.2665732204914093\n",
            "layers.4.0.weight has gradient mean of 1.3258540630340576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Transformer block\n",
        "Composed by multi-head attention, layer normalization, dropout, feed forward layers, GELU activation function"
      ],
      "metadata": {
        "id": "kCSQjh15mgt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.att = MultiHeadAttention(\n",
        "        d_in=cfg[\"emb_dim\"],\n",
        "        d_out=cfg[\"emb_dim\"],\n",
        "        context_length=cfg[\"context_length\"],\n",
        "        num_heads=cfg[\"n_heads\"],\n",
        "        dropout=cfg[\"drop_rate\"],\n",
        "        qkv_bias=cfg[\"qkv_bias\"]\n",
        "    )\n",
        "    self.ff = FeedForward(cfg)\n",
        "    # normalize the embedding dimension\n",
        "    self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    # save input for attention shortcut\n",
        "    shortcut = x\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    # shortcut for FNN\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "uUMtqgofmr30"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPT architecture class"
      ],
      "metadata": {
        "id": "LFoBv4N2vsI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "    self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "    *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "    self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(\n",
        "        cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "    )\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, seq_len = in_idx.shape\n",
        "    tok_embeds = self.tok_emb(in_idx)\n",
        "    pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "    x = tok_embeds + pos_embeds\n",
        "    x = self.drop_emb(x)\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "6oTfYoC1vuWR"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run model"
      ],
      "metadata": {
        "id": "dWnhVdBGyfPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "out = model(batch)\n",
        "print(\"Input batch:\\n\", batch)\n",
        "print(\"\\nOutput shape:\", out.shape)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-aKqfZnygSa",
        "outputId": "075df479-50ca-47a5-8afa-1a31f617d40c"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch:\n",
            " tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n",
            "\n",
            "Output shape: torch.Size([2, 4, 50257])\n",
            "tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],\n",
            "         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],\n",
            "         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],\n",
            "         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],\n",
            "\n",
            "        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],\n",
            "         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],\n",
            "         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],\n",
            "         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute total number of trainable/trained parameters"
      ],
      "metadata": {
        "id": "LqarEVGSX7aL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IoO4FG9X1ew",
        "outputId": "ac7753a8-14a9-44ac-e34c-65ecbfbe7be3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 163,009,536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generation of possibly multiple tokens, whose length depends on the user.Multiple iteration of the generation of a next token permit the model to generate meaninful complete sentences.\n",
        "\n",
        "Approaches:\n",
        "1. **softmax** -> the model returns everytime the token with the highest probability. The result is always the same every time the model is used.\n",
        "\n",
        "2. **multinomial** -> the model returns most of the time the token with the highest probability, but not always. In this way, the originality of the output text is improved.\n",
        "\n",
        "3. **temperature scaling** -> divide logitcs by a value greater than 0. Higher is the value of teh temperature, higher will be the originality. Too high values are not recommended (> 2/3), since we can have nonsensical output test\n",
        "\n",
        "**top-k approach**: possibly used before temperature scaling or multinomial to avoid nonsensical text -> select top k tokens with highest probabilities after softmax, and set to -inf all other logits s.t. the resulting probability with the softmax is 0 for all the other tokens, then normalize to have all remaining probabilities summed up to 1 (automatically done using the softmax).\n"
      ],
      "metadata": {
        "id": "BmgAOTOMi1ws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple version without text originality (used in the model traning)"
      ],
      "metadata": {
        "id": "IgGgjx98TDb2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# idx -> input with size (batch, token ids)\n",
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "  for _ in range(max_new_tokens):\n",
        "    # crops current context size to supported context size by the model -> if model supports size 5 and context size is 10, maintain last 5 tokens as context\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "\n",
        "    # output of model is (batch, token, vocab), thus take last token\n",
        "    logits = logits[:, -1, :]\n",
        "\n",
        "    # temperature scaling\n",
        "\n",
        "    # temperature = 0.5\n",
        "    # logits = logits/temperature\n",
        "\n",
        "    probas = torch.softmax(logits, dim=-1)\n",
        "\n",
        "    # get idx of the next token that will be part of the next input given to the model\n",
        "    idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "    # multinomial option -> take one sample and convert to tensor\n",
        "    # idx_text = torch.multinomial(probas, num_sample=1).item()\n",
        "\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "  return idx\n"
      ],
      "metadata": {
        "id": "hWotAydHf9bT"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete version with text originality (not used in the model training)"
      ],
      "metadata": {
        "id": "0qi3wzoaTEtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "  for _ in range(max_new_tokens):\n",
        "    idx_cond = idx[:, -context_size:]\n",
        "    with torch.no_grad():\n",
        "      logits = model(idx_cond)\n",
        "    logits = logits[:, -1, :]\n",
        "    # choose if possible top k tokens\n",
        "    if top_k is not None:\n",
        "      top_logits, _ = torch.topk(logits, top_k)\n",
        "      min_val = top_logits[:, -1]\n",
        "      # set all other logits to -inf\n",
        "      logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device),logits)\n",
        "\n",
        "    if temperature > 0.0:\n",
        "      logits = logits / temperature\n",
        "      probs = torch.softmax(logits, dim=-1)\n",
        "      idx_next = torch.multinomial(probs, num_samples=1)\n",
        "    else:\n",
        "      idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "    if idx_next == eos_id:\n",
        "      break\n",
        "\n",
        "    idx = torch.cat((idx, idx_next), dim=1)\n",
        "  return idx\n"
      ],
      "metadata": {
        "id": "E1LLvag4TKWt"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = \"Hello, I am\"\n",
        "encoded = tokenizer.encode(start_context)\n",
        "# add batch dimension (instead of (4), we get (1,4))\n",
        "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "\n",
        "# put model in eval mode to remove layers used only for training (e.g. dropout layers)\n",
        "model.eval()\n",
        "\n",
        "out = generate_text_simple(\n",
        "  model=model,\n",
        "  idx=encoded_tensor,\n",
        "  max_new_tokens=6,\n",
        "  context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output:\", out)\n",
        "print(\"Output length:\", len(out[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9N0Z3QI3kWlN",
        "outputId": "007aff35-d2fb-4563-9fa6-cb33f1ee5075"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
            "Output length: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conversion from GPT output to token\n",
        "The result of GPT model is a matrix. To get the predicted token, it is required to extract the last vector, apply the softmax to get probabilities, identify the index associated with the highest probability which is also the token ID, and get the token given its specific ID."
      ],
      "metadata": {
        "id": "nAEsA6tClLCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
        "print(decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSDQ5g1vlC1R",
        "outputId": "10beb81d-79ba-463b-9e56-a02c9a17b838"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, I am Featureiman Byeswickattribute argue\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretraining"
      ],
      "metadata": {
        "id": "vDlw791vbaj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# shorter context_length\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 256,\n",
        "    \"emb_dim\": 768,\n",
        "    \"n_heads\": 12,\n",
        "    \"n_layers\": 12,\n",
        "    \"drop_rate\": 0.1,\n",
        "    \"qkv_bias\": False\n",
        "}\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwNCnu99ea2g",
        "outputId": "dc044e4e-3050-4b43-8e19-66bb4840af77"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(256, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions to convert from tokens to tokens IDs and opposite"
      ],
      "metadata": {
        "id": "P1hMfooScnlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "  encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "  # insert batch dimension\n",
        "  encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "  return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "  # remove batch dimension\n",
        "  flat = token_ids.squeeze(0)\n",
        "  return tokenizer.decode(flat.tolist())"
      ],
      "metadata": {
        "id": "qn5Gi8IIct6a"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try functions"
      ],
      "metadata": {
        "id": "ayYPSjrQepXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = \"Every effort moves you\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model = model,\n",
        "    idx = text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens = 10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amptzUZyeqWd",
        "outputId": "b881532a-b850-4e64-917c-a59dd5015663"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you rentingetic wasn refres RexMeCHicular stren\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss computation without training\n",
        "Load small dataset 'The verdict' to save time"
      ],
      "metadata": {
        "id": "nHl2tYYblXgP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"the-verdict.txt\"\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "  text_data = f.read()"
      ],
      "metadata": {
        "id": "ceYmvl3ZlciV"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "get text length"
      ],
      "metadata": {
        "id": "zRBMCBPBlobo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_chars = len(text_data)\n",
        "total_tok = tokenizer.encode(text_data)\n",
        "print(\"Total characters:\", total_chars)\n",
        "print(\"Total tokens:\", len(total_tok))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9k9Oisqlpvs",
        "outputId": "b559c0ba-b3be-4d1b-b8fc-d62ed039a769"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 20479\n",
            "Total tokens: 5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split dataset into training set and validation set with ratio of 90% and 10%"
      ],
      "metadata": {
        "id": "fbzK51VpmrN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(text_data))\n",
        "train_data = text_data[:split_idx]\n",
        "val_data = text_data[split_idx:]"
      ],
      "metadata": {
        "id": "6Dkzv9UBmwQS"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loader -> contains for each element input batch and target batch"
      ],
      "metadata": {
        "id": "Hx4OtxfcnjF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size = batch_size,\n",
        "      shuffle = shuffle,\n",
        "      drop_last = drop_last,\n",
        "      num_workers = num_workers\n",
        "  )\n",
        "\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "e7zNWXYLnkiD"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "Sjp8ShVhnmDc"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "dBJGKcRDntdw"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute loss function for a single batch"
      ],
      "metadata": {
        "id": "Njuc6quMo5MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "  # transferring data to a device permit to transfer data to GPU\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "  logits = model(input_batch)\n",
        "  # logits.flatten(0,1) -> merge dimensions 0 and 1 into a single one, from (batch_dim, nr_tokens, vocab_dim) to (batch_dim*nr_tokens, vocab_dim)\n",
        "  loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "KequA5g3o4Xs"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute loss for all batches of traning and validation set"
      ],
      "metadata": {
        "id": "NgLfo8DZwNAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches = None):\n",
        "  total_loss = 0.\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "  # if num_batches not specified\n",
        "  elif num_batches is None:\n",
        "    # dataloader returns data grouped by batches\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i < num_batches:\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      # sum loss over batches\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  # average loss over all batches\n",
        "  return total_loss/num_batches\n"
      ],
      "metadata": {
        "id": "0JhKxsBtwO5K"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute loss (high since no training performed yet)"
      ],
      "metadata": {
        "id": "OYf-UF8_9p0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device)\n",
        "  val_loss = calc_loss_loader(val_loader, model, device)\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ViLgvTO8lJy",
        "outputId": "db37985d-ad5b-4056-b1bc-e06d254c9f91"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.987583584255642\n",
            "Validation loss: 10.98110580444336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM Pretraining\n",
        "traning:\n",
        "1. iterate over each epoch (complete iteration over all input data)\n",
        "2. iterate over each batch\n",
        "3. reset loss gradients for each batch (batches are processed independently and their loss shouldn't be summed up).\n",
        "4. compute loss on current batch (forward pass)\n",
        "5. compute backpropagation (backward pass) to compute gradient of the loss with respect to the parameters (weights, biases)\n",
        "6. update parameters based on the gradients get in the previous step\n",
        "7. (print training and validation losses for tracking progress)\n",
        "\n",
        "Evaluating the validation set within the training process permit to understand how the model perform on unseen data in order to use techniques to improve precision if needed (modify hyperparameter such as batch size, learning rate, etc. ) and can implement strategies such as early stopping to avoid a biased model on training data (overfitting). In addition spikes in validation data indicates issues in the training process"
      ],
      "metadata": {
        "id": "yyKaxZo19T5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs, eval_freq, eval_iter, start_context, tokenizer):\n",
        "  # track losses and tokens seen\n",
        "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "  # global_step initialized with -1 so that it can work as index\n",
        "  tokens_seen, global_step = 0, -1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      # compute loss gradients\n",
        "      loss.backward()\n",
        "      # update model parameters depending on loss gradients\n",
        "      optimizer.step()\n",
        "      tokens_seen += input_batch.numel()\n",
        "      global_step += 1\n",
        "\n",
        "      # eval_freq is the frequency on which we evaluate the model on a validation or test set (if 5, each 5 batches we evaluate the model), optional to see improvements\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        track_tokens_seen.append(tokens_seen)\n",
        "\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "              f\"Train loss {train_loss:.3f}, \"\n",
        "              f\"Val loss {val_loss:.3f}\"\n",
        "              )\n",
        "\n",
        "    generate_and_print_sample(model, tokenizer, device, start_context)\n",
        "  return train_losses, val_losses, track_tokens_seen\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7fLmADJC9Tdn"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation: compute loss over training and evaluation sets ensuring model is in evaluation mode without dropout  "
      ],
      "metadata": {
        "id": "UcVW_1q1JGM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "  model.eval()\n",
        "  # not required in evaluation and save computational space\n",
        "  with torch.no_grad():\n",
        "    # compute loss for multiple batches to have more stable evaluation\n",
        "    train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    # swap again to training mode for the training process in train_model_simple()\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "4MFjp0QJGeoc"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model by taking a text snippet and passing to the model"
      ],
      "metadata": {
        "id": "ePvsVqp0v0ai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start_context = text snippet for evaluation purposes\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "  model.eval()\n",
        "  # in the positional embedding weights the first shape is the context size\n",
        "  context_size = model.pos_emb.weight.shape[0]\n",
        "  encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "  with torch.no_grad():\n",
        "    # use the model in order to get the predicted next tokens starting from start_context\n",
        "    token_ids = generate_text_simple(model=model, idx=encoded,max_new_tokens=50, context_size=context_size)\n",
        "  decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "  print(decoded_text.replace(\"\\n\", \" \"))\n",
        "  model.train()"
      ],
      "metadata": {
        "id": "6yhf8ig7xvXE"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model"
      ],
      "metadata": {
        "id": "FIaRTY0D0RHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(\n",
        "     model.parameters(),\n",
        "    lr=0.0004, weight_decay=0.1\n",
        ")\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer,\n",
        "    device,num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0agY0tuT0SZf",
        "outputId": "46b940f4-2b4f-4d56-ceef-280e5d5e59f7"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.783, Val loss 9.927\n",
            "Ep 1 (Step 000005): Train loss 7.985, Val loss 8.335\n",
            "Every effort moves you,,,,,,,,,,,,.                                     \n",
            "Ep 2 (Step 000010): Train loss 6.753, Val loss 7.048\n",
            "Ep 2 (Step 000015): Train loss 6.114, Val loss 6.573\n",
            "Every effort moves you, and,, and, and,,,,, and, and,,,,,,,,,,,,,, and,,,, and,, and,,,,, and,,,,,,\n",
            "Ep 3 (Step 000020): Train loss 5.525, Val loss 6.490\n",
            "Ep 3 (Step 000025): Train loss 5.324, Val loss 6.387\n",
            "Every effort moves you, and to the picture.                      \"I, and the of the of the's the honour, and, and I had been, and I\n",
            "Ep 4 (Step 000030): Train loss 4.761, Val loss 6.360\n",
            "Ep 4 (Step 000035): Train loss 4.461, Val loss 6.258\n",
            "Every effort moves you of the to the picture--as of the picture--as I had been \" it was his \" I was the     \"I was his I had been the his pictures--and it the picture and I had been the picture of\n",
            "Ep 5 (Step 000040): Train loss 3.833, Val loss 6.196\n",
            "Every effort moves you know the \"Oh, and he was not the fact by his last word.         \"I was.      \"Oh, I felt a little a little the    \n",
            "Ep 6 (Step 000045): Train loss 3.352, Val loss 6.139\n",
            "Ep 6 (Step 000050): Train loss 2.861, Val loss 6.112\n",
            "Every effort moves you know; and my dear, and he was not the fact with a little of the house of the fact of the fact, and.                       \n",
            "Ep 7 (Step 000055): Train loss 2.347, Val loss 6.138\n",
            "Ep 7 (Step 000060): Train loss 2.084, Val loss 6.179\n",
            "Every effort moves you know,\" was one of the picture for nothing--I told Mrs.  \"I looked--as of the fact, and I felt him--his back his head to the donkey. \"Oh, and_--because he had always _\n",
            "Ep 8 (Step 000065): Train loss 1.521, Val loss 6.176\n",
            "Ep 8 (Step 000070): Train loss 1.272, Val loss 6.178\n",
            "Every effort moves you?\" \"I didn't bear the picture--I told me.  \"I looked up, and went on groping and Mrs. I was back the head to look up at the honour being _mine_--because he was when I\n",
            "Ep 9 (Step 000075): Train loss 1.000, Val loss 6.277\n",
            "Ep 9 (Step 000080): Train loss 0.718, Val loss 6.281\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 10 (Step 000085): Train loss 0.506, Val loss 6.325\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to the donkey again. I saw that, and down the room, when I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot training and validation loss -> the model overfit due to the small dataset and the absence of techniques to improve it, but this is a very basic training only for demonstration purposes"
      ],
      "metadata": {
        "id": "RRGUPINcJfn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "  ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "  ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "  ax1.set_xlabel(\"Epochs\")\n",
        "  ax1.set_ylabel(\"Loss\")\n",
        "  ax1.legend(loc=\"upper right\")\n",
        "  ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "  ax2 = ax1.twiny()\n",
        "  ax2.plot(tokens_seen, train_losses, alpha=0)\n",
        "  ax2.set_xlabel(\"Tokens seen\")\n",
        "  fig.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "DRzAOPRMJca0"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9P_9tlULJ39F",
        "outputId": "d3f6a920-031f-473d-843e-3fc04f96605c"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model in order to not execute the training everytime (computational advantage) such that even if the session is closed the model will be still available. Two options:\n",
        "1. save the model only -> if we don't perform any additional training (only inference)\n",
        "2. save the model and the optimizer -> if additional training is performed, the optimizer contains important parameters that are needed in order to have a correct convergence"
      ],
      "metadata": {
        "id": "wqLmwvNhpVT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # save onyl the model\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "\n",
        "# # save the model and the optimizer\n",
        "# torch.save({\n",
        "#     \"model_state_dict\": model.state_dict(),\n",
        "#     \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "#     },\n",
        "#     \"model_and_optimizer.pth\"\n",
        "# )"
      ],
      "metadata": {
        "id": "vAF0eVwrpifv"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model (in general this is done in another notebook in order to not run the training again)"
      ],
      "metadata": {
        "id": "OKnhK_LWrgFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"model.pth\", map_location=device)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
        "\n",
        "# checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
        "# model = GPTModel(GPT_CONFIG_124M)\n",
        "# model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "# optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "# # if we save also the optimizer, this means we want to train it again thus set train state\n",
        "# model.train();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4o_L2-IrpcQ",
        "outputId": "929bb10b-efa0-4cfa-e73c-58cda3b952c8"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-110-38373285fd4a>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\"model.pth\", map_location=device)\n",
            "<ipython-input-110-38373285fd4a>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load pretrained weights of GPT2\n",
        "Since if the device is computationally limited the training is not effective, it is possible to load the weights of the GPT-2 models obtained during pretraining."
      ],
      "metadata": {
        "id": "4ACJJiensUu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get gpt_download.py\n",
        "\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split('/')[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNTOtHbGsUes",
        "outputId": "2adfdec6-90ef-4966-bfc0-30f5668feae3"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x7e0407dacfd0>)"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load GPT-2 architecture settings and parameters"
      ],
      "metadata": {
        "id": "VeEKbSxttyul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrFd1PJUzELf",
        "outputId": "cf6af62b-c8a4-4898-c771-94e5cc0e9db2"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Settings:\", settings)\n",
        "print(\"Parameter dictionary keys:\", params.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TU2iNEDxWrC",
        "outputId": "c64a2bc4-e070-4052-8750-0ae00577f690"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
            "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All configurations for GPT-2 models (small, medium, large, xl)"
      ],
      "metadata": {
        "id": "RrBwdvQ1vD6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_configs = {\n",
        "  \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "  \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "  \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "  \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}"
      ],
      "metadata": {
        "id": "043ICH82vIzd"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update our previous config ('GPT_CONFIG_124M') with new parameters"
      ],
      "metadata": {
        "id": "m65inDalvP8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"gpt2-small (124M)\"\n",
        "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
        "NEW_CONFIG.update(model_configs[model_name])\n",
        "\n",
        "NEW_CONFIG.update({\"context_length\": 1024})\n",
        "# not used anymore but need to match the settings employed\n",
        "NEW_CONFIG.update({\"qkv_bias\": True})"
      ],
      "metadata": {
        "id": "F-6a68mNvVtG"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "create new GPT instance with new parameters, however the initial weights are random"
      ],
      "metadata": {
        "id": "h3OIegbWwN5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt = GPTModel(NEW_CONFIG)\n",
        "gpt.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWVUmYN9wQll",
        "outputId": "b5b018aa-1403-4663-91bc-aa16b6718fa1"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to check that left and right tensors have same shape (check if assignment of data is correct)"
      ],
      "metadata": {
        "id": "5Bk4jGCjwuDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "  if left.shape != right.shape:\n",
        "    raise ValueError(f\"Shape mismatch. Left: {left.shape}, \" \"Right: {right.shape}\")\n",
        "  return torch.nn.Parameter(torch.tensor(right))"
      ],
      "metadata": {
        "id": "Sks2FRY6w7wt"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign loaded parameters to the new model with the new configuration. Complex function and based on some guessing since GPT used different names. It is possible to load the pretrained GPT model without manually assign all parameters, but this is for demonstration purpose (i.e. model = GPT2Model.from_pretrained(\"gpt2\"))"
      ],
      "metadata": {
        "id": "wIaw5i4pxAJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_weights_into_gpt(gpt, params):\n",
        "  # assign positional and token embeddings\n",
        "  gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "  gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "  # iteration over all transformer blocks\n",
        "  for b in range(len(params[\"blocks\"])):\n",
        "    # query, key and value components for self attention mechanism\n",
        "    # split concatenated weights for q,k,v into 3 components q_w, k_w, v_w, transpose and assign\n",
        "    q_w, k_w, v_w = np.split(\n",
        "        (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "    gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "        gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "    gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "        gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "    gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "        gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "    # same for biases\n",
        "    q_b, k_b, v_b = np.split(\n",
        "        (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "    gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "        gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "    gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "        gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "    gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "        gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "    # output projection\n",
        "    gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "        gpt.trf_blocks[b].att.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "    gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "        gpt.trf_blocks[b].att.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "    # feedforward nn\n",
        "    gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "    gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "    gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "    gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "    # layer normalization, where scale='g' for gain and shift='b' for bias\n",
        "    gpt.trf_blocks[b].norm1.scale = assign(\n",
        "        gpt.trf_blocks[b].norm1.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "    gpt.trf_blocks[b].norm1.shift = assign(\n",
        "        gpt.trf_blocks[b].norm1.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "    gpt.trf_blocks[b].norm2.scale = assign(\n",
        "        gpt.trf_blocks[b].norm2.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "    gpt.trf_blocks[b].norm2.shift = assign(\n",
        "        gpt.trf_blocks[b].norm2.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "  # final layer normalization and output head\n",
        "  gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "  gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "  gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "p-4eY21-xGqY"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_weights_into_gpt(gpt, params)\n",
        "gpt.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl1nvY9eSsk0",
        "outputId": "bf50b7f0-3947-4580-94fe-8cdfa7f46c8d"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use model with loaded parameters to see token prediction and check that output text is coherent."
      ],
      "metadata": {
        "id": "BYuIlWe3S2JM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "token_ids = generate(\n",
        "model=gpt,\n",
        "idx=text_to_token_ids(\"Every effort moves you\", tokenizer).to(device), max_new_tokens=25,\n",
        "context_size=NEW_CONFIG[\"context_length\"],\n",
        "top_k=50,\n",
        "temperature=1.5\n",
        ")\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GGj1TNCS0lx",
        "outputId": "aca6b0d4-6bbf-4cb8-cdbe-2c17b6fe5619"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " Every effort moves you toward an equal share for each vote plus half. Inequality is often not an accurate representation of human worth; to know the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning for classification (spam detection)\n",
        "In general LLMs are not used for tasks such as spam detection, however this is for demonstration purposes with a simple task.\n",
        "\n",
        "After the pretraining where the model learns how to perdict next tokens in a coherent way, LLMs can be fine-tuned to perform required task such as classification, question answering, sentiment analysis, text generation, etc. The fine-tuning can be divided into:\n",
        "1. fine-tuning for classification: perform classification given an input such as spam detection, element recognition given images (tumors, object detection), text classification, etc. More specific than other fine-tuning since it is specialized in performing only the classification it saw only during the training phase\n",
        "\n",
        "2. fine-tuning to follow instruction: fine-tune model such that given some instructions (in form of prompts) in natural language it is able to understand and execute the required different tasks. E.g. \"**Translate in German** 'text to translate' \". Can perform different tasks"
      ],
      "metadata": {
        "id": "jFDTH4KYUMX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preprocessing"
      ],
      "metadata": {
        "id": "9vIrDo7oYAW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset for spam detection"
      ],
      "metadata": {
        "id": "cIgvJVqMYc_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
        "# name of downloaded file\n",
        "zip_path = \"sms_spam_collection.zip\"\n",
        "# where the unzipped file is stored\n",
        "extracted_path = \"sms_spam_collection\"\n",
        "# create path pointing to sms_spam_collection appending SMSSpamCollection to the path -> after the unzip the dataset will be called SMSSpamCollection.tsv\n",
        "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "IOZ6BsM9YjUt"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path):\n",
        "  if data_file_path.exists():\n",
        "    print(f\"{data_file_path} already exists. Skipping download and extraction.\")\n",
        "    return\n",
        "  # download file (write in zip_path the file read from url contained in 'response')\n",
        "  with urllib.request.urlopen(url) as response:\n",
        "    with open(zip_path, \"wb\") as out_file:\n",
        "        out_file.write(response.read())\n",
        "  # unzip file\n",
        "  with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
        "    zip_ref.extractall(extracted_path)\n",
        "  # add .tsv file extension\n",
        "  original_file_path = Path(extracted_path) / \"SMSSpamCollection\"\n",
        "  # add .tsv extension\n",
        "  os.rename(original_file_path, data_file_path)\n",
        "  print(f\"File downloaded and saved as {data_file_path}\")\n"
      ],
      "metadata": {
        "id": "2VcteykjY0s7"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRdoCLeSb8x0",
        "outputId": "459b930d-8780-4abd-a26e-422434a2884e"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sms_spam_collection/SMSSpamCollection.tsv already exists. Skipping download and extraction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load .tsv file in a dataframe"
      ],
      "metadata": {
        "id": "LJ4BqAoVcsqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# /t since it it tab-separated (TSV) to know how to separate columns\n",
        "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"] )\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6C6Za6YWcyqC",
        "outputId": "c7f140e9-3063-45eb-994c-fa3b55d30bd4"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Label                                               Text\n",
              "0      ham  Go until jurong point, crazy.. Available only ...\n",
              "1      ham                      Ok lar... Joking wif u oni...\n",
              "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3      ham  U dun say so early hor... U c already then say...\n",
              "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
              "...    ...                                                ...\n",
              "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
              "5568   ham               Will  b going to esplanade fr home?\n",
              "5569   ham  Pity, * was in mood for that. So...any other s...\n",
              "5570   ham  The guy did some bitching but I acted like i'd...\n",
              "5571   ham                         Rofl. Its true to its name\n",
              "\n",
              "[5572 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38fb8dfe-9f3d-41df-880b-c04f0e593068\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5567</th>\n",
              "      <td>spam</td>\n",
              "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5568</th>\n",
              "      <td>ham</td>\n",
              "      <td>Will  b going to esplanade fr home?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5569</th>\n",
              "      <td>ham</td>\n",
              "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5570</th>\n",
              "      <td>ham</td>\n",
              "      <td>The guy did some bitching but I acted like i'd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5571</th>\n",
              "      <td>ham</td>\n",
              "      <td>Rofl. Its true to its name</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5572 rows  2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38fb8dfe-9f3d-41df-880b-c04f0e593068')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-38fb8dfe-9f3d-41df-880b-c04f0e593068 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-38fb8dfe-9f3d-41df-880b-c04f0e593068');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6016fa29-427e-4dec-81cf-fa9699ec02eb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6016fa29-427e-4dec-81cf-fa9699ec02eb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6016fa29-427e-4dec-81cf-fa9699ec02eb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_654d1aaf-5027-4924-b90a-b3746a98c380\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_654d1aaf-5027-4924-b90a-b3746a98c380 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5572,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"spam\",\n          \"ham\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5169,\n        \"samples\": [\n          \"K, makes sense, btw carlos is being difficult so you guys are gonna smoke while I go pick up the second batch and get gas\",\n          \"URGENT! Your mobile No *********** WON a \\u00a32,000 Bonus Caller Prize on 02/06/03! This is the 2nd attempt to reach YOU! Call 09066362220 ASAP! BOX97N7QP, 150ppm\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0et9--zdPp1",
        "outputId": "59ac90ab-88b9-4c34-be79-dcce4a936737"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     4825\n",
            "spam     747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple dataset balancing since the two classes 'spam' and 'ham' have very different sizes -> done to avoid class bias and improve generalization (not performant with the small classes)"
      ],
      "metadata": {
        "id": "4D3tfHpslvUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_balanced_dataset(df):\n",
        "  num_spam = df[df[\"Label\"] == 'spam'].shape[0]\n",
        "  # sample randomly num_spam number of ham data to balance the two classes. Ensure reproducibility with random_state\n",
        "  ham_subset = df[df[\"Label\"] == 'ham'].sample(num_spam, random_state=123)\n",
        "  # concatenate new column of the ham subset with all spam\n",
        "  balanced_df = pd.concat([ham_subset, df[df[\"Label\"] == \"spam\"]])\n",
        "  return balanced_df"
      ],
      "metadata": {
        "id": "paj_M9cImLuu"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df = create_balanced_dataset(df)\n",
        "print(balanced_df[\"Label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3pzR57roimi",
        "outputId": "3259b446-b1cf-4ee8-9fee-55619433d12b"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "ham     747\n",
            "spam    747\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert labels into integers 0 and 1"
      ],
      "metadata": {
        "id": "imCPtGGMIj7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})"
      ],
      "metadata": {
        "id": "6wbcFAAlImme"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split data in dataset randomly in training set (70%), validation set (10%), test set (20%)"
      ],
      "metadata": {
        "id": "wxu0wDbmJA2S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def random_split(df, train_frac, validation_frac):\n",
        "  # sample randomly data, since we take frac=1, we simply shuffle them. Drop old indices and reset them\n",
        "  df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "  # get indices of split\n",
        "  train_end = int(len(df) * train_frac)\n",
        "  validation_end = train_end + int(len(df) * validation_frac)\n",
        "\n",
        "  train_df = df[:train_end]\n",
        "  validation_df = df[train_end:validation_end]\n",
        "  test_df = df[validation_end:]\n",
        "\n",
        "  return train_df, validation_df, test_df\n"
      ],
      "metadata": {
        "id": "fQner_naJJRS"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)"
      ],
      "metadata": {
        "id": "fYIM2OE2V1aG"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save datasets as CSV"
      ],
      "metadata": {
        "id": "7XoMIrKaWLga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.to_csv(\"train.csv\", index=None)\n",
        "validation_df.to_csv(\"validation.csv\", index=None)\n",
        "test_df.to_csv(\"test.csv\", index=None)"
      ],
      "metadata": {
        "id": "iTdrTPP6WNRk"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create data loaders -> for the pretraining we had a sliding window that permit to had sentences of the same length, but in emails the lengths are different, thus:\n",
        "1. truncate all messages to the length of the shortest one -> cheaper but there is the possibility of information loss if the shortest message is really short\n",
        "\n",
        "2. add padding to all messages reaching the length of the longest one -> expensive but preserve information -> in this case we use this option with \"<|endoftext|>\" as padding token\n"
      ],
      "metadata": {
        "id": "0ohvFfZXXw-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamDataset(Dataset):\n",
        "  def __init__(self, csv_file, tokenizer, max_length=None, pad_token_id=50256):\n",
        "    self.data = pd.read_csv(csv_file)\n",
        "    self.encoded_texts = [tokenizer.encode(text) for text in self.data[\"Text\"]]\n",
        "    if max_length is None:\n",
        "      self.max_length = self._longest_encoded_length()\n",
        "    else:\n",
        "      self.max_length = max_length\n",
        "    self.encoded_texts = [encoded_text[:self.max_length]for encoded_text in self.encoded_texts]\n",
        "    self.encoded_texts = [encoded_text + [pad_token_id] *(self.max_length - len(encoded_text))for encoded_text in self.encoded_texts]\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    encoded = self.encoded_texts[index]\n",
        "    label = self.data.iloc[index][\"Label\"]\n",
        "    return (torch.tensor(encoded, dtype=torch.long), torch.tensor(label, dtype=torch.long))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "  def _longest_encoded_length(self):\n",
        "    max_length = 0\n",
        "    for encoded_text in self.encoded_texts:\n",
        "      encoded_length = len(encoded_text)\n",
        "      if encoded_length > max_length:\n",
        "        max_length = encoded_length\n",
        "    return max_length\n"
      ],
      "metadata": {
        "id": "gfmqR9TboOpt"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pad train, test and validation sets"
      ],
      "metadata": {
        "id": "KhCzmupO0mQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SpamDataset(\n",
        "    csv_file=\"train.csv\",\n",
        "    max_length=None,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "edEhm1oGrrUV"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = SpamDataset(\n",
        "    csv_file=\"validation.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "GvQNoJut3NT8"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SpamDataset(\n",
        "    csv_file=\"test.csv\",\n",
        "    max_length=train_dataset.max_length,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "93oL_Vqz3Os7"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.max_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev3AyWdE0xoU",
        "outputId": "ccf44bfe-e750-4d82-b4ea-1e9e3b9e723d"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate data loader -> each batch contains 8 input, where an input is an email text of 120 tokens and the label is 0 or 1 depending if spam or not"
      ],
      "metadata": {
        "id": "lyFMtRq_3giC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 0\n",
        "batch_size = 8\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0PHvz6l3hnF",
        "outputId": "5071a697-eb3b-4190-da8f-90181302306d"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e043df16ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=True,\n",
        ")"
      ],
      "metadata": {
        "id": "xRjr85Ty3nRM"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ],
      "metadata": {
        "id": "PTh7YtOe3k2f"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=num_workers,\n",
        "    drop_last=False,\n",
        ")"
      ],
      "metadata": {
        "id": "G1baThSn3lx_"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check batch sizes if correct"
      ],
      "metadata": {
        "id": "blnUTZYQ4Gsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_batch, target_batch in train_loader:\n",
        "  pass\n",
        "print(\"Input batch dimensions:\", input_batch.shape)\n",
        "print(\"Label batch dimensions\", target_batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8xHbSxc4I4K",
        "outputId": "020b43fa-c105-4884-94a0-219492cafbdd"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch dimensions: torch.Size([8, 120])\n",
            "Label batch dimensions torch.Size([8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print total batches in each set"
      ],
      "metadata": {
        "id": "Rx095dOMKuUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{len(train_loader)} training batches\")\n",
        "print(f\"{len(val_loader)} validation batches\")\n",
        "print(f\"{len(test_loader)} test batches\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep7HHTwnKwLD",
        "outputId": "9ae47789-78ca-4b0f-ac82-70920d993ede"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130 training batches\n",
            "19 validation batches\n",
            "38 test batches\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model setup"
      ],
      "metadata": {
        "id": "a_p6SeN8YJHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model initialization"
      ],
      "metadata": {
        "id": "YN0EN1moUwBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same configuration as for unlabeled data"
      ],
      "metadata": {
        "id": "sue2EVgeU2zD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
        "INPUT_PROMPT = \"Every effort moves\""
      ],
      "metadata": {
        "id": "SOT3O2YcUydI"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,\n",
        "    \"context_length\": 1024,\n",
        "    \"drop_rate\": 0.0,\n",
        "    \"qkv_bias\": True\n",
        "}"
      ],
      "metadata": {
        "id": "9Sem6hreU1Ov"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}"
      ],
      "metadata": {
        "id": "QqLL9VcaU6yt"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])"
      ],
      "metadata": {
        "id": "tiQ2NUdtU_vp"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download weights into gpt-2 model"
      ],
      "metadata": {
        "id": "GuuUBtCJVLWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5eC2I_8VMzz",
        "outputId": "1c450623-2a72-4728-c82e-6ff920813c39"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/124M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/124M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/124M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/124M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/124M/vocab.bpe\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTModel(\n",
              "  (tok_emb): Embedding(50257, 768)\n",
              "  (pos_emb): Embedding(1024, 768)\n",
              "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
              "  (trf_blocks): Sequential(\n",
              "    (0): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (1): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (2): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (3): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (4): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (5): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (6): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (7): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (8): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (9): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (10): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (11): TransformerBlock(\n",
              "      (att): MultiHeadAttention(\n",
              "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (layers): Sequential(\n",
              "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (1): GELU()\n",
              "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "      (norm1): LayerNorm()\n",
              "      (norm2): LayerNorm()\n",
              "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (final_norm): LayerNorm()\n",
              "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if model outputs coherent text, thus the parameters has been loaded correctly"
      ],
      "metadata": {
        "id": "9s4pbaxAV6gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = \"Every effort moves you\"\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(text_1, tokenizer),\n",
        "    max_new_tokens=15,\n",
        "    context_size=BASE_CONFIG[\"context_length\"]\n",
        ")\n",
        "print(token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpLoS0YFV--Z",
        "outputId": "83a0fc5d-5e5a-47bb-cb10-50e1b207d7cb"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Every effort moves you forward.\n",
            "\n",
            "The first step is to understand the importance of your work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification head\n",
        "Substitute the original output layer that maps to the vocabulary ('out_head') with a smaller output layer that maps to two classes only: 0 and 1 (not spam or spam). Without fine-tuning, with the new output layer the output of the model will be (batch_size, nr_tokens, vocab_dim), but the vocab_dim instead of being 50257 is now 2 (embedding dim) (for prediction of next token we take into consideration only the last token of the output).\n",
        "\n",
        "We freeze the model for the fine-tuning in order to not update the parameters -> keep the pre-trained weights intact and only train the new layers or the final layer to adapt the model to a new task or dataset"
      ],
      "metadata": {
        "id": "BHNA4TP-WETQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False"
      ],
      "metadata": {
        "id": "rTh5uM_qWGZS"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change final layer. This one has requires_grad = True automatically, thus this will be the only layer updated during fine-tuning. However despite the fact this should be sufficient, by empirical experiments fine-tuning additional layers improve the performance of the model"
      ],
      "metadata": {
        "id": "JvjGqfKdYF9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "num_classes = 2\n",
        "\n",
        "model.out_head = torch.nn.Linear(\n",
        "    # = 768, output dimension of transformer blocks\n",
        "    in_features = BASE_CONFIG[\"emb_dim\"],\n",
        "    out_features = num_classes,\n",
        ")"
      ],
      "metadata": {
        "id": "ynOZjchuYHcR"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make trainable last transformer block and final LayerNorm"
      ],
      "metadata": {
        "id": "izxjQfBzZAOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.trf_blocks[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "for param in model.final_norm.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "Jtz9MJ4PZfK4"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider only last token of the output for the classification. Why? Given the implementation of the attention mask that permit each token to have information about all preceeding tokens in the sequence, the last one is the one with most information. Using the softmax on these two values for each input text we get the probability to be spam or not spam, and we take the highest one as the correct class"
      ],
      "metadata": {
        "id": "ZFk6jfjhcIDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement evaluation utilities\n",
        "Compute classification loss and model accuracy (percentage of correct predictions)"
      ],
      "metadata": {
        "id": "gvrbmOmzr2IN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_accuracy_loader(data_loader, model, device, num_batches=None):\n",
        "  model.eval()\n",
        "  correct_predictions, num_examples = 0, 0\n",
        "  if num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "      num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i < num_batches:\n",
        "      input_batch = input_batch.to(device)\n",
        "      target_batch = target_batch.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        logits = model(input_batch)[:, -1, :]\n",
        "      predicted_labels = torch.argmax(logits, dim=-1)\n",
        "      num_examples += predicted_labels.shape[0]\n",
        "\n",
        "      correct_predictions += ((predicted_labels == target_batch).sum().item())\n",
        "    else:\n",
        "      break\n",
        "  return correct_predictions / num_examples"
      ],
      "metadata": {
        "id": "hDe0GO1Bsy-S"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function loss: cross-entropy for a single batch"
      ],
      "metadata": {
        "id": "v73MwIiQwpvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "  input_batch = input_batch.to(device)\n",
        "  target_batch = target_batch.to(device)\n",
        "  logits = model(input_batch)[:, -1, :]\n",
        "  loss = torch.nn.functional.cross_entropy(logits, target_batch)\n",
        "  return loss"
      ],
      "metadata": {
        "id": "UT5wsEN2wrlw"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loss for all batches"
      ],
      "metadata": {
        "id": "6kvhtEUtw3ue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "  total_loss = 0.\n",
        "  if len(data_loader) == 0:\n",
        "    return float(\"nan\")\n",
        "  elif num_batches is None:\n",
        "    num_batches = len(data_loader)\n",
        "  else:\n",
        "    num_batches = min(num_batches, len(data_loader))\n",
        "  for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "    if i < num_batches:\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      total_loss += loss.item()\n",
        "    else:\n",
        "      break\n",
        "  return total_loss / num_batches"
      ],
      "metadata": {
        "id": "QFv8CLmLw5bF"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial loss"
      ],
      "metadata": {
        "id": "G9NvGuHuxWOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "\n",
        "val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "test_loss = calc_loss_loader(test_loader, model, device, num_batches=5)\n",
        "\n",
        "print(f\"Training loss: {train_loss:.3f}\")\n",
        "print(f\"Validation loss: {val_loss:.3f}\")\n",
        "print(f\"Test loss: {test_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZoOun9UxXKU",
        "outputId": "1f7774c4-6365-4d93-dc56-b1ad944b2852"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 2.183\n",
            "Validation loss: 2.583\n",
            "Test loss: 2.322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model fine-tuning and usage"
      ],
      "metadata": {
        "id": "LvlA9GHsYMZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier_simple(model, train_loader, val_loader, optimizer, device,num_epochs, eval_freq, eval_iter):\n",
        "  train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "  examples_seen, global_step = 0, -1\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for input_batch, target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      examples_seen += input_batch.shape[0]\n",
        "      global_step += 1\n",
        "\n",
        "      if global_step % eval_freq == 0:\n",
        "        train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "              f\"Train loss {train_loss:.3f}, \"\n",
        "              f\"Val loss {val_loss:.3f}\")\n",
        "\n",
        "    train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "    val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "\n",
        "    print(f\"Training accuracy: {train_accuracy*100:.2f}% | \", end=\"\")\n",
        "    print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "    train_accs.append(train_accuracy)\n",
        "    val_accs.append(val_accuracy)\n",
        "  return train_losses, val_losses, train_accs, val_accs, examples_seen\n"
      ],
      "metadata": {
        "id": "BRZDmypE219M"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run fine-tuning computing the time needed"
      ],
      "metadata": {
        "id": "bI_Kw6udBrSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
        "num_epochs = 5"
      ],
      "metadata": {
        "id": "a7LV2lbFB9bh"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
        "        model, train_loader, val_loader, optimizer, device,\n",
        "        num_epochs=num_epochs, eval_freq=50,\n",
        "        eval_iter=5\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time)/60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6E-Sgjn2A3WY",
        "outputId": "47e304a9-3af1-44ee-dc88-5cf259861a18"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.153, Val loss 2.392\n",
            "Ep 1 (Step 000050): Train loss 0.617, Val loss 0.637\n",
            "Ep 1 (Step 000100): Train loss 0.523, Val loss 0.557\n",
            "Training accuracy: 70.00% | Validation accuracy: 72.50%\n",
            "Ep 2 (Step 000150): Train loss 0.561, Val loss 0.489\n",
            "Ep 2 (Step 000200): Train loss 0.419, Val loss 0.397\n",
            "Ep 2 (Step 000250): Train loss 0.409, Val loss 0.353\n",
            "Training accuracy: 82.50% | Validation accuracy: 85.00%\n",
            "Ep 3 (Step 000300): Train loss 0.333, Val loss 0.320\n",
            "Ep 3 (Step 000350): Train loss 0.340, Val loss 0.306\n",
            "Training accuracy: 90.00% | Validation accuracy: 90.00%\n",
            "Ep 4 (Step 000400): Train loss 0.136, Val loss 0.200\n",
            "Ep 4 (Step 000450): Train loss 0.153, Val loss 0.132\n",
            "Ep 4 (Step 000500): Train loss 0.222, Val loss 0.137\n",
            "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
            "Ep 5 (Step 000550): Train loss 0.207, Val loss 0.143\n",
            "Ep 5 (Step 000600): Train loss 0.083, Val loss 0.074\n",
            "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
            "Training completed in 60.24 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_values(epochs_seen, examples_seen, train_values, val_values, label=\"loss\"):\n",
        "  fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "  # plot training and validation values with respect to epochs\n",
        "  ax1.plot(epochs_seen, train_values, label=f\"Training {label}\")\n",
        "  ax1.plot(epochs_seen, val_values, label=f\"Validation {label}\")\n",
        "  ax1.set_xlabel(\"Epoch\")\n",
        "  ax1.set_ylabel(label.capitalize())\n",
        "  ax1.legend()\n",
        "\n",
        "  # create new x-axis on same plot that overlay x1 but without interfere\n",
        "  ax2 = ax1.twiny()\n",
        "  ax2.plot(examples_seen, train_values, alpha=0)\n",
        "  ax2.set_xlabel(\"Examples seen\")\n",
        "\n",
        "  # adjust layout to avoid overlap between plot elements\n",
        "  fig.tight_layout()\n",
        "  # store plot\n",
        "  plt.savefig(f\"{label}-plot.pdf\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "dgUrbgGJEdsf"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot classification loss"
      ],
      "metadata": {
        "id": "dN5Uu_c2EaUw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Pr4D1ShzdRfj",
        "outputId": "1a35ac65-273b-470d-8907-5f88ac3522a2"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot classification accuracy"
      ],
      "metadata": {
        "id": "7gb8-rKnDQl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_accs))\n",
        "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_accs))\n",
        "plot_values(epochs_tensor, examples_seen_tensor, train_accs, val_accs, label=\"accuracy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "o8HoG5W8DSMo",
        "outputId": "e5e0f980-23c1-4a53-a25f-66fbd3af53c7"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute accuracy of entire datasets"
      ],
      "metadata": {
        "id": "AV74-UoVvpdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
        "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
        "test_accuracy = calc_accuracy_loader(test_loader, model, device)"
      ],
      "metadata": {
        "id": "qPCU52gevlq7"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
        "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
        "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UakE9UaLwORn",
        "outputId": "4b790840-338b-4104-b9ec-a94fc1abd9b8"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy: 97.21%\n",
            "Validation accuracy: 97.32%\n",
            "Test accuracy: 95.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use model for email classification with new data"
      ],
      "metadata": {
        "id": "pVbkL1B7w_hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_review(text, model, tokenizer, device, max_length=None, pad_token_id=50256):\n",
        "  model.eval()\n",
        "  input_ids = tokenizer.encode(text)\n",
        "  # gives the maximum sequence length that can be processed by the model (the one chosen as maximum length for the fine-tuning)\n",
        "  supported_context_length = model.pos_emb.weight.shape[1]\n",
        "  # truncate text if needed\n",
        "  input_ids = input_ids[:min(max_length, supported_context_length)]\n",
        "  # add padding\n",
        "  input_ids += [pad_token_id] * (max_length - len(input_ids))\n",
        "\n",
        "  input_tensor = torch.tensor(input_ids, device=device).unsqueeze(0)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # get last token with all information\n",
        "    logits = model(input_tensor)[:, -1, :]\n",
        "  # get value (0,1) associated with highest probability of last token\n",
        "  predicted_label = torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "  return \"spam\" if predicted_label == 1 else \"not spam\"\n"
      ],
      "metadata": {
        "id": "l3SThihxxDcU"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check prediction"
      ],
      "metadata": {
        "id": "veZgqr_Y09Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_1 = (\n",
        "    \"You are a winner you have been specially\"\n",
        "    \" selected to receive $1000 cash or a $2000 award.\"\n",
        ")\n",
        "\n",
        "text_2 = (\n",
        "    \"Hey, just wanted to check if we're still on\"\n",
        "    \" for dinner tonight? Let me know!\"\n",
        ")"
      ],
      "metadata": {
        "id": "vhcDhkI8yrKT"
      },
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classify_review(text_1, model, tokenizer, device, max_length= train_dataset.max_length))\n",
        "print(classify_review(text_2, model, tokenizer, device, max_length=train_dataset.max_length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7EVH4thytLq",
        "outputId": "0ff6a2f8-1506-44c8-db43-efbc43127197"
      },
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spam\n",
            "not spam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model"
      ],
      "metadata": {
        "id": "v3LoCpFV062O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"review_classifier.pth\")"
      ],
      "metadata": {
        "id": "nzAWnEZC06hQ"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning to follow instruction\n",
        "The model should be able to understand a given instruction as prompt and get a correct answer."
      ],
      "metadata": {
        "id": "6nS2NqLl1aCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset preprocessing"
      ],
      "metadata": {
        "id": "KPL9fxvB2WNU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset consists of 1,100 instructionresponse pairs"
      ],
      "metadata": {
        "id": "8qujdfDk2iO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_load_file(file_path, url):\n",
        "  # if not already downloaded\n",
        "    if not os.path.exists(file_path):\n",
        "        with urllib.request.urlopen(url) as response:\n",
        "            text_data = response.read().decode(\"utf-8\")\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(text_data)\n",
        "    # otherwise read only\n",
        "    else:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text_data = file.read()\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "    return data"
      ],
      "metadata": {
        "id": "HQh9uB4_2i3p"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"instruction-data.json\"\n",
        "url = (\"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\")\n",
        "data = download_and_load_file(file_path, url)\n",
        "print(\"Number of entries:\", len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSUVChz13Kbh",
        "outputId": "9f39abd8-7ea6-43e5-b9f4-43bbfd7b0a22"
      },
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of entries: 1100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get structure of data, composed by 'instruction', 'input' and 'output', however 'input' is not always present, e.g. {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}"
      ],
      "metadata": {
        "id": "PwFTOIkA3Txx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example entry:\\n\", data[50])\n",
        "print(\"Example entry:\\n\", data[599])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QQkX5wj3Tge",
        "outputId": "3c056a61-00c4-4834-fdbd-bbc4696f0c35"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example entry:\n",
            " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n",
            "Example entry:\n",
            " {'instruction': 'Create a new sentence by combining the two sentences.', 'input': 'He likes to swim. He goes to the pool every day.', 'output': 'He likes to swim and goes to the pool every day.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two methods to format the entries for the LLM:\n",
        "1. **Alpaca prompt style**\n",
        "\n",
        "  \"Below is an instruction that\n",
        "  describes a task. Write a response\n",
        "  that appropriately completes the\n",
        "  request.\n",
        "\n",
        "  \\### Instruction:\n",
        "\n",
        "  Identify the correct spelling of the\n",
        "  following word.\n",
        "\n",
        "  \\### Input:\n",
        "\n",
        "  Ocassion\n",
        "\n",
        "  \\### Response:\n",
        "\n",
        "  The correct spelling is 'Occasion'.\"\n",
        "\n",
        "2. **Phi-3 prompt style**\n",
        "\n",
        "  \"<|user|>\n",
        "\n",
        "  Identify the correct spelling of the\n",
        "  following word: 'Ocassion'\n",
        "\n",
        "  <|assistant|>\n",
        "\n",
        "  The correct spelling is 'Occasion'.\""
      ],
      "metadata": {
        "id": "XdGja15hcKq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert entries in Alpaca format"
      ],
      "metadata": {
        "id": "qFvRpkmodwjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(entry):\n",
        "  instruction_text = (\n",
        "      f\"Below is an instruction that describes a task. \"\n",
        "      f\"Write a response that appropriately completes the request.\"\n",
        "      f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "  )\n",
        "\n",
        "  input_text = (\n",
        "      f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "  )\n",
        "\n",
        "  return instruction_text + input_text"
      ],
      "metadata": {
        "id": "q7puFhr6dz_p"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check format"
      ],
      "metadata": {
        "id": "77EWo6UNdwXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = format_input(data[50])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9xryyAbekYH",
        "outputId": "341c9b30-c093-436f-80ca-273c04ea33c2"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Identify the correct spelling of the following word.\n",
            "\n",
            "### Input:\n",
            "Ocassion\n",
            "\n",
            "### Response:\n",
            "The correct spelling is 'Occasion.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split dataset into train (85%), test(10%) and validation set(5%)"
      ],
      "metadata": {
        "id": "MsxHaV4ofRLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_portion = int(len(data) * 0.85)\n",
        "test_portion = int(len(data) * 0.1)\n",
        "val_portion = len(data) - train_portion - test_portion\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]\n",
        "\n",
        "print(\"Train set size:\", len(train_data))\n",
        "print(\"Test set size:\", len(test_data))\n",
        "print(\"Val set size:\", len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgCGzlN1fQ2c",
        "outputId": "d247a1c7-aa1a-4246-c982-a95f58cb82ee"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 935\n",
            "Test set size: 110\n",
            "Val set size: 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Organize data into batches.\n",
        "Steps:\n",
        "1. Format data using prompt template -> Format input into an instruction- response template.\n",
        "\n",
        "2. Tokenize formatted data -> Convert instruction-response entry into token IDs.\n",
        "3. Adjust to the same length with padding tokens -> Add end-of-text tokens (50256) to pad data samples to the same length.\n",
        "\n",
        "4. Create target token IDs for training -> Create a list of target token IDs for the model to learn (these are the inputs shifted by 1, plus an additional padding token).\n",
        "\n",
        "5. Replace padding tokens with placeholders (masking for ignoring) -> Replace certain padding tokens by -100 to exclude them from the training loss. Another possibility in addition is to mask the input instruction (instruction + input) replacing the IDs with -100 leaving the response only in order to reduce overfitting , but this has been proven to reduce performance\n",
        "\n",
        "Previsouly the collate function (how data are combined in the batches) was implemented automatically by Pytorch in the class DataLoader, however for instruction fine-tuning we had to create it on our own"
      ],
      "metadata": {
        "id": "RH1VErtQgAQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1 and 2\n",
        "\n",
        "Configure dataset in correct format and encode"
      ],
      "metadata": {
        "id": "sJyhjGqaoEhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InstructionDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer):\n",
        "    self.data = data\n",
        "    self.encoded_texts = []\n",
        "    for entry in data:\n",
        "      instruction_plus_input = format_input(entry)\n",
        "      response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "      full_text = instruction_plus_input + response_text\n",
        "      self.encoded_texts.append(tokenizer.encode(full_text))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      return self.encoded_texts[index]\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.data)"
      ],
      "metadata": {
        "id": "QL5vctU1nzS-"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 -> use a custom collate function such that each input in the batch has same length adding a padding ID 50256 corresponding to the tag '<|endoftext|>', however the length of the inputs in each batch can vary in order to avoid adding too much padding (the padding added depends on the longest sequence in the current batch):\n",
        "\n",
        "**Batch 1**\n",
        "\n",
        "input 1 [1,2,3,4,5] -> input 1 [1,2,3,4,5]\n",
        "\n",
        "input 2 [1,2,3] -> input 2 [1,2,3,50256,50256]\n",
        "\n",
        "input 3 [1,2] -> input 3 [1,2,50256,50256,50256]\n",
        "\n",
        "\n",
        "**Batch 2**\n",
        "\n",
        "input 1 [1,2] -> input 1 [1,2,50256]\n",
        "\n",
        "input 2 [1,2,3] -> input 2 [1,2,3]\n",
        "\n",
        "input 3 [1] -> input 3 [1,50256,50256]"
      ],
      "metadata": {
        "id": "6h_4eWGHo3kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get single batch as input\n",
        "def custom_collate_draft_1(batch,pad_token_id=50256,device=\"cpu\"):\n",
        "  # max length sequence for each batch\n",
        "  batch_max_length = max(len(item)+1 for item in batch)\n",
        "  inputs_lst = []\n",
        "\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    new_item += [pad_token_id]\n",
        "\n",
        "    padded = (new_item + [pad_token_id] *(batch_max_length - len(new_item)))\n",
        "    inputs = torch.tensor(padded[:-1])\n",
        "    inputs_lst.append(inputs)\n",
        "\n",
        "  # convert list of inputs to tensor\n",
        "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "  return inputs_tensor"
      ],
      "metadata": {
        "id": "lN_cQq4ksSTC"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "print(custom_collate_draft_1(batch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owUYH6lLuRXv",
        "outputId": "fc76f7d2-4226-4699-a8ab-319728873031"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4\n",
        "\n",
        "Collate function updated to generate target token ID, that are the inputs shifted by 1 as in the pretraining -> in this way the model is able to generate coherent tokens and follow its flow since the structure of instruction, input and response is given in the input, i.e.\n",
        "\n",
        "Input:    Translate 'hello' into Spanish:\n",
        "\n",
        "Target:   Translate 'hello' into Spanish: hola\n",
        "\n",
        "In our case since the inputs are padded, the target is the input sequence without the first token, and with an additional 50256 ID at the end (end tag). Despite the fact there is not an actual text token, the model is able to understand the relationship between tokens and how to generate coherent next token to follow instructions."
      ],
      "metadata": {
        "id": "VyiLK66LG9ed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_draft_2(batch,pad_token_id=50256,device=\"cpu\"):\n",
        "  batch_max_length = max(len(item)+1 for item in batch)\n",
        "  inputs_lst, targets_lst = [], []\n",
        "  for item in batch:\n",
        "    new_item = item.copy()\n",
        "    new_item += [pad_token_id]\n",
        "\n",
        "  padded = (new_item + [pad_token_id] *(batch_max_length - len(new_item)))\n",
        "  inputs = torch.tensor(padded[:-1])\n",
        "  targets = torch.tensor(padded[1:])\n",
        "  inputs_lst.append(inputs)\n",
        "  targets_lst.append(targets)\n",
        "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "  targets_tensor = torch.stack(targets_lst).to(device)\n",
        "  return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "DnzcOOk9IyCz"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets = custom_collate_draft_2(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZv2a3IjMnkx",
        "outputId": "86f742f1-c750-4be6-af39-df3100ad3e07"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    8,     9, 50256, 50256, 50256]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5\n",
        "\n",
        "Substitute all 50256 IDs with -100 (masking out padding tokens -> value ignored by the cross-entropy loss because by implementation it ignores targets labeled -100) but not the last one, since in this way the model is able to learn when the response should be terminated. This process is not performed in the fine-tuning for classification since we train only on the last output token"
      ],
      "metadata": {
        "id": "oF4SMV0DM1fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch,pad_token_id=50256,ignore_index=-100,allowed_max_length=None,device=\"cpu\"):\n",
        "  batch_max_length = max(len(item)+1 for item in batch)\n",
        "  inputs_lst, targets_lst = [], []\n",
        "  for item in batch:\n",
        "      new_item = item.copy()\n",
        "      new_item += [pad_token_id]\n",
        "\n",
        "      padded = (new_item + [pad_token_id] *(batch_max_length - len(new_item)))\n",
        "      inputs = torch.tensor(padded[:-1])\n",
        "      targets = torch.tensor(padded[1:])\n",
        "\n",
        "      # create a mask where padding tokens are `True`, e.g. mask = tensor([False, False, False, True, True, True])\n",
        "      mask = targets == pad_token_id\n",
        "      # find indices of the padding tokens, e.g. indices = tensor([3,4,5])\n",
        "      indices = torch.nonzero(mask).squeeze()\n",
        "      # check if more than one pad_token_id\n",
        "      if indices.numel() > 1:\n",
        "        # set all but not first pad token to -100\n",
        "        targets[indices[1:]] = ignore_index\n",
        "\n",
        "      # truncate if max length given\n",
        "      if allowed_max_length is not None:\n",
        "        inputs = inputs[:allowed_max_length]\n",
        "        targets = targets[:allowed_max_length]\n",
        "\n",
        "      inputs_lst.append(inputs)\n",
        "      targets_lst.append(targets)\n",
        "\n",
        "  inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "  targets_tensor = torch.stack(targets_lst).to(device)\n",
        "  return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "GeWiL32HM1Qb"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets = custom_collate_fn(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNVdDY5hXhhz",
        "outputId": "719adc6c-6a0d-4ac2-a354-5a4d0889de6c"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "prwbKOntkR71"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create new function based on customized_collate_fn with some pre-defined parameters that will be freezed"
      ],
      "metadata": {
        "id": "elELqEZqkk0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "customized_collate_fn = partial(\n",
        "    custom_collate_fn,\n",
        "    device=device,\n",
        "    allowed_max_length=1024\n",
        ")"
      ],
      "metadata": {
        "id": "hlBGoXVMkeUG"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loader"
      ],
      "metadata": {
        "id": "brRl8wxik4hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_workers = 0\n",
        "batch_size = 8\n",
        "torch.manual_seed(123)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QkY-HPzk5kU",
        "outputId": "982eb03a-053a-4c4c-d1ab-b7a442be7d4e"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e043df16ed0>"
            ]
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)"
      ],
      "metadata": {
        "id": "Pni1Gn7Xk95I"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "metadata": {
        "id": "NXXeo1PIlK__"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check sizes (dimensions 1 are different in batches because of the custom collate function for padding)"
      ],
      "metadata": {
        "id": "n9BZXuGLmQqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train loader:\")\n",
        "i = 0\n",
        "for inputs, targets in train_loader:\n",
        "  # if i < 10:\n",
        "    print(inputs.shape, targets.shape)\n",
        "  # i+=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx3j2zqLl9rI",
        "outputId": "9b86d3f0-09ad-4133-af8c-0a87055b8cce"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loader:\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 73]) torch.Size([8, 73])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 79]) torch.Size([8, 79])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 89]) torch.Size([8, 89])\n",
            "torch.Size([8, 59]) torch.Size([8, 59])\n",
            "torch.Size([8, 88]) torch.Size([8, 88])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 76]) torch.Size([8, 76])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 58]) torch.Size([8, 58])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 87]) torch.Size([8, 87])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 71]) torch.Size([8, 71])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 65]) torch.Size([8, 65])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 60]) torch.Size([8, 60])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 57]) torch.Size([8, 57])\n",
            "torch.Size([8, 72]) torch.Size([8, 72])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 62]) torch.Size([8, 62])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 70]) torch.Size([8, 70])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 80]) torch.Size([8, 80])\n",
            "torch.Size([8, 81]) torch.Size([8, 81])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 82]) torch.Size([8, 82])\n",
            "torch.Size([8, 63]) torch.Size([8, 63])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 68]) torch.Size([8, 68])\n",
            "torch.Size([8, 67]) torch.Size([8, 67])\n",
            "torch.Size([8, 77]) torch.Size([8, 77])\n",
            "torch.Size([8, 91]) torch.Size([8, 91])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 61]) torch.Size([8, 61])\n",
            "torch.Size([8, 75]) torch.Size([8, 75])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 78]) torch.Size([8, 78])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 64]) torch.Size([8, 64])\n",
            "torch.Size([8, 83]) torch.Size([8, 83])\n",
            "torch.Size([8, 66]) torch.Size([8, 66])\n",
            "torch.Size([8, 74]) torch.Size([8, 74])\n",
            "torch.Size([8, 69]) torch.Size([8, 69])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tuning"
      ],
      "metadata": {
        "id": "eKpJIer92ZBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load pretrained medium GPT model for better performance"
      ],
      "metadata": {
        "id": "EBftRYZFmg8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "settings, params = download_and_load_gpt2(model_size=model_size,models_dir=\"gpt2\")\n",
        "\n",
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tVPptxAmiG1",
        "outputId": "f44b1be0-18e5-459a-84e7-1201e81562f5"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File already exists and is up-to-date: gpt2/355M/checkpoint\n",
            "File already exists and is up-to-date: gpt2/355M/encoder.json\n",
            "File already exists and is up-to-date: gpt2/355M/hparams.json\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.data-00000-of-00001\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.index\n",
            "File already exists and is up-to-date: gpt2/355M/model.ckpt.meta\n",
            "File already exists and is up-to-date: gpt2/355M/vocab.bpe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute initial loss"
      ],
      "metadata": {
        "id": "27qqkf2kolhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "torch.manual_seed(123)\n",
        "with torch.no_grad():\n",
        "  train_loss = calc_loss_loader(train_loader, model, device, num_batches=5)\n",
        "  val_loss = calc_loss_loader(val_loader, model, device, num_batches=5)\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "v9oev4hSomif",
        "outputId": "66f77746-2947-4685-ab82-43b549a5d8b1"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "0D or 1D target tensor expected, multi-target not supported",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-188-978b904382eb>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-154-f10846f2446a>\u001b[0m in \u001b[0;36mcalc_loss_loader\u001b[0;34m(data_loader, model, device, num_batches)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_loss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m       \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-153-4af242e1324b>\u001b[0m in \u001b[0;36mcalc_loss_batch\u001b[0;34m(input_batch, target_batch, model, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mtarget_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3480\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tuning"
      ],
      "metadata": {
        "id": "hobHm_D-sz2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
        "num_epochs = 2\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "JLL-E4PysgY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot train and validation losses"
      ],
      "metadata": {
        "id": "NXttdXOJs6C5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "id": "Y1R3iv-Qs9CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM evaluation"
      ],
      "metadata": {
        "id": "UpQL63N52ar0"
      }
    }
  ]
}